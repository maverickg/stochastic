%&subfile
\begin{document}
\ifstandalonechapter\setcounter{chapter}{1}\fi
\chapter{Brownian motion}
\section{Scaling limit of random walks.}
Our first goal is to understand \emph{Brownian motion}, which is used to model ``noisy fluctuations'' of stocks, and various other objects.
This is named after the botanist Robert Brown, who observed that the microscopic movement of pollen grains appears random.
Intuitively, Brownian motion can be thought of as a process that performs a random walk in continuous time.

We begin by describing Brownian motion as the scaling limit of discrete random walks.
Let $X_1$, $X_2$, \dots, be a sequence of i.i.d.\ random variables which take on the values $\pm 1$ with probability $1/2$.
Define the time interpolated random walk $S(t)$ by setting $S(0) = 0$, and
\begin{equation}\label{e:S}
  S(t) = S(n) + (t - n) X_{n+1}
  \quad\text{when } t \in (n, n+1]\,.
\end{equation}
Note $S(n) = \sum_{1}^n X_i$, and so at integer times $S$ is simply a symmetric random walk with step size $1$.

Our aim now is to rescale $S$ so that it takes a random step at shorter and shorter time intervals, and then take the limit.
In order to get a meaningful limit, we will have to compensate by also scaling the step size.
Let $\epsilon > 0$ and define
\begin{equation}\label{e:Sep}
  S_\epsilon(t) = \alpha_\epsilon S\paren[\Big]{ \frac{t}{\epsilon} }\,,
\end{equation}
where $\alpha_\epsilon$ will be chosen below in a manner that ensures convergence of $S_\epsilon(t)$ as $\epsilon \to 0$.
Note that $S_\epsilon$ now takes a random step of size $\alpha_\epsilon$ after every $\epsilon$ time units.

To choose $\alpha_\epsilon$, we compute the variance of $S_\epsilon$.
Note first
\begin{equation*}
  \var S(t) = \floor{t} + (t - \floor{t})^2,
\end{equation*}
and\footnote{%
  Here $\floor{x}$ denotes the greatest integer smaller than $x$. That is, $\floor{x} = \max\set{ n \in \Z \st n \leq x }$.
}
consequently
\begin{equation*}
  \var S_\epsilon(t)
    = \alpha_\epsilon^2 \paren[\Big]{
	\floor[\Big]{\frac{t}{\epsilon}}
	+ \paren[\Big]{\frac{t}{\epsilon} - \floor[\Big]{\frac{t}{\epsilon}}}^2
    }\,.
\end{equation*}
In order to get a ``nice limit'' of $S_\epsilon$ as $\epsilon \to 0$, one would at least expect that $\var S_\epsilon(t)$ converges as $\epsilon \to 0$.
From the above, we see that choosing
\begin{equation*}
  \alpha_\epsilon = \sqrt{\epsilon}
\end{equation*}
immediately implies
\begin{equation*}
  \lim_{\epsilon \to 0} \var S_\epsilon(t) = t\,.
\end{equation*}

\begin{theorem}\label{t:SRW}
  The processes $S_\epsilon(t) \defeq \sqrt{\epsilon} S( t / \epsilon )$ ``converge'' as $\epsilon \to 0$.
  The limiting process, usually denoted by $W$, is called a (standard, one dimensional) \emph{Brownian motion}.
\end{theorem}

The proof of this theorem uses many tools from the modern theory of probability, and is beyond the scope of this course.
The important thing to take away from this is that Brownian motion can be well approximated by a random walk that takes steps of variance $\epsilon$ on a time interval of size $\epsilon$.
 

\section{A crash course in measure theoretic probability.}

Each of the random variables $X_i$ can be adequately described by finite probability spaces.
The collection of all $X_i$'s can not be, but is still ``intuitive enough'' to to be understood using the tools from discrete probability.
The limiting process $W$, however, can not be adequately described using tools from ``discrete'' probability: For each $t$, $W(t)$ is a continuous random variable, and the collection of all $W(t)$ for $t \geq 0$ is an \emph{uncountable} collection of \emph{correlated} random variables.
This process is best described and studied through measure theoretic probability, which is very briefly described in this section.

\begin{definition}
  The \emph{sample space} $\Omega$ is simply a non-empty set.
\end{definition}

\begin{definition}
  A \emph{$\sigma$-algebra} $\mathcal G \subseteq \mathcal P(\Omega)$ is a \emph{non-empty} collection of \emph{subsets} of $\Omega$ which is:
  \begin{enumerate}
    \item closed under compliments (i.e. if $A \in \mathcal G$, then $A^c \in \mathcal G$),
    \item and closed under \emph{countable} unions (i.e. $A_1$, $A_2$, \dots are all elements of $\mathcal G$, then the union $\cup_1^\infty A_i$ is also an element of $\mathcal G$).
  \end{enumerate}
  Elements of the $\sigma$-algebra are called \emph{events}, or $\mathcal G$-measurable events.
\end{definition}

\begin{remark}
  The notion of $\sigma$-algebra is \emph{central} to probability, and represents \emph{information}.
  Elements of the $\sigma$-algebra are events whose probability are known.
\end{remark}

\begin{remark}
  You should check that the above definition implies that $\emptyset, \Omega \in \mathcal G$, and that $\mathcal G$ is also closed under countable intersections and set differences.
\end{remark}

\begin{definition}
  A \emph{probability measure} on $(\Omega, \mathcal G)$ is a \emph{countably additive} function $\P\colon \mathcal G \to [0, 1]$ such that $\P(\Omega) = 1$.
  That is, for each $A \in \mathcal G$, $\P(A) \in [0, 1]$ and $\P(\Omega) = 1$.
  Moreover, if $A_1,\ A_2, \dots \in \mathcal G$ are \emph{pairwise disjoint}, then
  \begin{equation*}
    \P\paren[\Big]{ \bigcup_{i = 1}^\infty A_i } = \sum_{i=1}^\infty \P(A_i)\,.
  \end{equation*}
  The triple $(\Omega, \mathcal G, \P)$ is called a \emph{probability space}.
\end{definition}

\begin{remark}
For a $\mathcal G$-measurable event $A$, $\P(A)$ represents the probability of the event $A$ occurring.
\end{remark}

\begin{remark}
  You should check that the above definition implies:
  \begin{enumerate}
    \item
      $\P(\emptyset) = 0$,
    \item
      If $A, B \in \mathcal G$ are disjoint, then $\P(A \cup B) = \P(A) + \P(B)$.
    \item
      $\P(A^c) = 1 - \P(A)$.
      More generally, if $A, B \in \mathcal G$ with $A \subseteq B$, then $\P(B - A) = \P(B) - \P(A)$.
    \item
      If $A_1 \subseteq A_2 \subseteq A_3 \cdots$ and each $A_i \in \mathcal G$ then $\P(\cup A_i) = \lim_{n \to \infty} \P(A_n)$.
    \item
      If $A_1 \supseteq A_2 \supseteq A_3 \cdots$ and each $A_i \in \mathcal G$ then $\P(\cap A_i) = \lim_{n \to \infty} \P(A_n)$.
  \end{enumerate}
\end{remark}

\begin{definition}
  A \emph{random variable} is a function $X:\Omega \to \R$ such that for every $\alpha \in \R$, the set $\set{ \omega \in \Omega \st X(\omega) \leq \alpha }$ is an element of $\mathcal G$.
  (Such functions are also called \emph{$\mathcal G$-measurable}, \emph{measurable with respect to $\mathcal G$}, or simply \emph{measurable} if the $\sigma$-algebra in question is clear from the context.)
\end{definition}
\begin{remark}
  The argument $\omega$ is \emph{always} suppressed when writing random variables.
  That is, the event $\set{\omega \in \Omega \st X(\omega) \leq \alpha}$ is simply written as $\set{X \leq \alpha}$.
\end{remark}
\begin{remark}
  Note for any random variable, $\set{X > \alpha} = \set{X \leq \alpha}^c$ which must also belong to $\mathcal G$ since $\mathcal G$ is closed under complements.
  You should check that for every $\alpha < \beta \in \R$ the events $\set{X < \alpha}$, $\set{X \geq \alpha}$, $\set{X > \alpha}$, $\set{X \in (\alpha, \beta)}$, $\set{X \in [\alpha, \beta)}$, $\set{X \in (\alpha, \beta]}$ and $\set{X \in (\alpha, \beta)}$ are all also elements of $\mathcal G$.

  Since $\P$ is defined on all of $\mathcal G$, the quantity $\P( \set{X \in (\alpha, \beta)})$ is mathematically well defined, and represents the chance that the random variable $X$ takes values in the interval $(\alpha, \beta)$.
  For brevity, I will often omit the outermost curly braces and write $\P(X \in (\alpha, \beta))$ for $\P(\set{X \in (\alpha, \beta)})$.
\end{remark}

\begin{remark}
  You should check that if $X$, $Y$ are random variables then so are $X \pm Y$, $XY$, $X/Y$ (when defined), $\abs{X}$, $X \varmin Y$ and $X \varmax Y$.
  In fact if $f: \R \to \R$ is any reasonably nice (more precisely, a Borel measurable) function, $f(X)$ is also a random variable.
\end{remark}

\begin{example}
  If $A \subseteq \Omega$, define $\one_A\colon \Omega \to \R$ by $\one_A(\omega) = 1$ if $\omega \in A$ and $0$ otherwise.
  Then $\one_A$ is a ($\mathcal G$-measurable) random variable if and only if $A \in \mathcal G$.
\end{example}

\begin{example}
  For $i \in \N$,  $a_i \in \R$ and $A_i \in \mathcal G$ be such that $A_i \cap A_j = \emptyset$ for $i \neq j$, and define
  \begin{equation*}
    X\defeq \sum_{i=1}^\infty a_i \one_{A_i}\,.
  \end{equation*}
  Then $X$ is a ($\mathcal G$-measurable) random variable.
  (Such variables are called \emph{simple} random variables.)
\end{example}

Note that if the $a_i$'s above are all distinct, then $\set{X = a_i} = A_i$, and hence $\sum_i a_i \P(X = a_i) = \sum_i a_i \P(A_i)$, which agrees with our notion of expectation from discrete probability.

\begin{definition}
  For the simple random variable $X$ defined above, we define \emph{expectation of $X$} by
  \begin{equation*}
    \E X = \sum_{i = 1}^\infty a_i \P(A_i)\,.
  \end{equation*}
\end{definition}

For general random variables, we define the expectation by approximation.
\begin{definition}
  If $Y$ is a nonnegative random variable, define
  \begin{equation*}
    \E Y \defeq \lim_{n \to \infty} \E X_n
    \quad\text{where }
    X_n \defeq \sum_{k=0}^{n^2-1} \frac{k}{n} \one_{\set{\frac{k}{n} \leq Y < \frac{k+1}{n}}}\,.
  \end{equation*}
\end{definition}
\begin{remark}
  Note each $X_n$ above is simple, and we have previously defined the expectation of simple random variables.
\end{remark}
\begin{definition}
  If $Y$ is any (not necessarily nonnegative) random variable, set $Y^+ = Y \varmax 0$ and $Y^- = Y \varmin 0$, and define the expectation by
  \begin{equation*}
    \E Y = \E Y^+ - \E Y^-\,,
  \end{equation*}
  provided at least one of the terms on the right is finite.
\end{definition}
\begin{remark}
  The expectation operator defined above is the \emph{Lebesgue integral} of $Y$ with respect to the probability measure $\P$, and is often written as
  \begin{equation*}
    \E Y = \int_\Omega Y \, d\P\,.
  \end{equation*}
  More generally, if $A \in \mathcal G$ we define
  \begin{equation*}
    \int_A Y \, d\P \defeq \E \paren{ \one_{A} Y} \,,
  \end{equation*}
  and when $A = \Omega$ we will often omit writing it.
\end{remark}

\begin{proposition}[Linearity]
  If $\alpha \in \R$ and $X, Y$ are random variables, then $\E(X + \alpha Y) = \E X + \alpha \E Y$.
\end{proposition}

\begin{proposition}[Positivity]
  If $X \geq 0$ almost surely, then $\E X \geq 0$
  Moreover, if $X > 0$ almost surely, $\E X > 0$.
  Consequently, (using linearity) if $X \leq Y$ almost surely then $\E X \leq \E Y$.
\end{proposition}
\begin{remark}
  By $X \geq 0$ almost surely, we mean that $\P(X \geq 0) = 1$.
\end{remark}

The proof of positivity is immediate, however the proof of linearity is surprisingly not as straightforward as you would expect.
It's easy to verify linearity for simple random variables, of course.
For general random variables, however, you need an approximation argument which requires either the \emph{dominated} or \emph{monotone} convergence theorem which guarantee $\lim \E X_n = \E \lim X_n$, under modest assumptions.
Since discussing these results at this stage will will lead us too far astray, we invite the curious to look up their proofs in any standard measure theory book.
The main point of this section was to introduce you to a framework which is capable of describing and studying the objects we will need for the remainder of the course.

\section{A first characterization of Brownian motion.}

We introduced Brownian motion by calling it a certain scaling limit of a simple random walk.
While this provides good intuition as to what Brownian motion actually is, it is a somewhat unwieldy object to work with.
Our aim here is to provide an intrinsic characterization of Brownian motion, that is both useful and mathematically convenient.

\begin{definition}\label{d:bm1}
  A Brownian motion is a \emph{continuous process} that has stationary independent increments.
\end{definition}

We will describe what this means shortly.
While this is one of the most intuitive definitions of Brownian motion, most authors choose to prove this as a theorem, and use the following instead.

\begin{definition}
  A Brownian motion is a \emph{continuous process} $W$ such that:
  \begin{enumerate}
    \item
       $W$ has independent increments, and
    \item
      For $s < t$, $W(t) - W(s) \sim N(0, \sigma^2 (t - s))$.
  \end{enumerate}
\end{definition}
\begin{remark}
  A \emph{standard} (one dimensional) Brownian motion is one for which $W(0) = 0$ and $\sigma = 1$.
\end{remark}

Both these definitions are equivalent, thought the proof is beyond the scope of this course.
In order to make sense of these definitions we need to define the terms \emph{continuous process}, \emph{stationary increments}, and \emph{independent increments}.

\subsection{Continuous processes}
\begin{definition}
  A \emph{stochastic process} (aka \emph{process}) is a function $X\colon \Omega \times [0, \infty)$ such that for every time $t \in [0, \infty)$, the function $\omega \mapsto X(t, \omega)$ is a random variable.
  The $\omega$ variable is usually suppressed, and we will almost always use $X(t)$ to denote the random variable obtained by taking the slice of the function $X$ at time $t$.
\end{definition}

\begin{definition}
  A \emph{continuous process} (aka \emph{continuous stochastic process}) is a stochastic process $X$ such that for (almost) every $\omega \in \Omega$ the function $t \mapsto X(t, \omega)$ is a continuous function of $t$.
  That is, 
  \begin{equation*}
    \P\paren[\Big]{ \lim_{s \to t} X(s) = X(t) \text{ for every } t \in [0, \infty) }
    = 1\,.
  \end{equation*}
\end{definition}


The processes $S(t)$ and $S_\epsilon(t)$ defined in~\eqref{e:S} and~\eqref{e:Sep} are continuous, but the process
\begin{equation*}
  \tilde S(t) \defeq \sum_{n=0}^{\floor{t}} X_n \,,
\end{equation*}
is not.

In general it is not true that the limit of continuous processes is again continuous.
However, one can show that the limit of $S_\epsilon$ (with $\alpha_\epsilon = \sqrt{\epsilon}$ as above yields a continuous process.

\subsection{Stationary increments}

\begin{definition}
  A process $X$ is said to have stationary increments if the distribution of $X_{t+h} - X_t$ does not depend on $t$.
\end{definition}

For the process $S$ in~\eqref{e:S}, note that for $n \in \N$, $S(n+1) - S(n) = X_{n+1}$ whose distribution \emph{does not} depend on $n$ as the variables $\set{X_i}$ were chosen to be independent and identically distributed.
Similarly, $S(n+k) - S(n) = \sum_{n+1}^{n+k} X_i$ which has the same distribution as $\sum_1^{k} X_i$ and is independent of $n$.

However, if $t \in \R$ and is not necessarily an integer, $S(t + k) - S(t)$ will in general depend on $t$.
So the process $S$ (and also $S_\epsilon$) do not have stationary increments.

We claim, that the limiting process $W$ does have stationary (normally distributed) increments.
Suppose for some fixed $\epsilon > 0$, both $s$ and $t$ are multiples of $\epsilon$.
In this case
\begin{equation*}
  S_\epsilon(t) - S_\epsilon(s)
    \sim \sqrt{\epsilon} \sum_{i=1}^{\floor{t - s}/\epsilon}
	X_i
    \xrightarrow{\epsilon \to 0} N(0, t-s)\,,
\end{equation*}
by the central limit theorem.
If $s, t$ aren't multiples of $\epsilon$ as we will have in general, the first equality above is true up to a remainder which can easily be shown to vanish.

The above heuristic argument suggests that the limiting process $W$ (from Theorem~\ref{t:SRW}) satisfies $W(t) - W(s) \sim N(0, t - s)$.
This certainly has independent increments since $W(t + h) - W(t) \sim N(0, h)$ which is independent of $t$.
This is also the reason why the normal distribution is often pre-supposed when defining Brownian motion.

\subsection{Independent increments}

\begin{definition}
  A process $X$ is said to have \emph{independent increments} if for every finite sequence of times $0 \leq t_0 < t_1 \cdots < t_N$, the random variables $X(t_0)$, $X(t_1) - X(t_0)$, $X(t_2) - X(t_1)$, \dots, $X(t_N) - X(t_{N-1})$ are all jointly independent.
\end{definition}

Note again for the process $S$ in~\eqref{e:S}, the increments at \emph{integer times} are independent.
Increments at non-integer times are correlated, however, one can show that in the limit as $\epsilon \to 0$ the increments of the process $S_\epsilon$ become independent.

Since we assume the reader is familiar with independence from discrete probability, the above is sufficient to motivate and explain the given definitions of Brownian motion.
However, notion of independence is important enough that we revisit it from a measure theoretic perspective next.
This also allows us to introduce a few notions on $\sigma$-algebras that will be crucial later.

\subsection{Independence in measure theoretic probability}


\begin{definition}
  Let $X$ be a random variable on $(\Omega, \mathcal G, \P)$.
  Define $\sigma(X)$ to be the  $\sigma$-algebra generated by the events $\set{X \leq \alpha}$ for every $\alpha \in \R$.
  That is, $\sigma(X)$ is the \emph{smallest} $\sigma$-algebra which contains each of the events $\set{X \leq \alpha}$ for every $\alpha \in \R$.
\end{definition}

\begin{remark}
  The $\sigma$-algebra $\sigma(X)$ represents all the information one can learn by observing $X$.
  For instance, consider the following game: A card is drawn from a shuffled deck, and you win a dollar if it is red, and lose one if it is black.
  Now the likely hood of drawing any particular card is $1/52$.
  However, if you are blindfolded and only told the outcome of the game, you have no way to determine that each gard is picked with probability $1/52$.
  The only thing you will be able to determine is that red cards are drawn as often as black ones.

  This is captured by $\sigma$-algebra as follows.
  Let $\Omega = \set{1, \dots, 52}$ represent a deck of cards, $\mathcal G = \mathcal P(\Omega)$, and define $\P(A) = \operatorname{card}(A)/52$.
  Let $R = \set{1, \dots 26}$ represent the red cards, and $B = R^c$ represent the black cards.
  The outcome of the above game is now the random variable $X = \one_R - \one_B$, and you should check that $\sigma(X)$ is exactly $\set{ \emptyset, R, B, \Omega}$.
\end{remark}

We will use $\sigma$-algebras extensively but, as you might have noticed, we haven't developed any examples.
Infinite $\sigma$-algebras are ``hard'' to write down explicitly, and what one usually does in practice is specify a \emph{generating} family, as we did when defining $\sigma(X)$.
\begin{definition}
  Given a collection of sets $A_\alpha$, where $\alpha$ belongs to some (possibly infinite) index set $\mathcal A$, we define $\sigma(\set{A_\alpha})$ to be the \emph{smallest} $\sigma$-algebra that contains each of the sets $A_\alpha$.
\end{definition}

That is, if $\mathcal G = \sigma(\set{A_\alpha})$, then we must have each $A_\alpha \in \mathcal G$.
Since $\mathcal G$ is a $\sigma$-algebra, all sets you can obtain from these by taking complements, countable unions and countable intersections intersections must also belong to $\mathcal G$.%
\footnote{
  Usually $\mathcal G$ contains \emph{much more} than all countable unions, intersections and complements of the $A_\alpha$'s.
  You might think you could keep including all sets you generate using countable unions and complements and arrive at all of~$\mathcal G$.
  It turns out that to make this work, you will usually have to do this \emph{uncountably} many times!

  This won't be too important within the scope of these notes.
  However, if you read a rigorous treatment and find the authors using some fancy trick (using Dynkin systems or monotone classes) instead of a naive countable unions argument, then the above is the reason why.
}
The fact that $\mathcal G$ is the smallest $\sigma$-algebra containing each $A_\alpha$ also means that if $\mathcal G'$ is any other $\sigma$-algebra that contains each $A_\alpha$, then $\mathcal G \subseteq \mathcal G'$.


\begin{remark}
  The smallest $\sigma$-algebra under which $X$ is a random variable (under which $X$ is measurable) is exactly $\sigma(X)$.
  It turns out that $\sigma(X) = X^{-1}(\mathcal B) =  \set{X \in B \st B \in \mathcal B}$, where $\mathcal B$ is the \emph{Borel} $\sigma$-algebra on $\R$.
  Here $\mathcal B$ is the \emph{Borel} $\sigma$-algebra, defined to be the $\sigma$-algebra on $\R$ generated by all open intervals.
\end{remark}
\iffalse
\begin{definition}
  We say two random variables $X$ and $Y$ are independent if for every $A \in \sigma(X)$ and $B \in \sigma(Y)$, the events $A$ and $B$ are independent (i.e.\ $\P(A \cap B) = P(A) P(B)$, or $P(A \given B) = P(A)$).
\end{definition}
\fi
\begin{definition}
  We say the random variables $X_1$, \dots, $X_N$ are independent if for every $i \in \set{1 \dots N}$ and every $A_i \in \sigma(X_i)$ we have
  \begin{equation*}
    \P\paren[\big]{ A_1 \cap A_2 \cap \cdots \cap A_N } = \P(A_1) \, \P(A_2) \cdots \P(A_N) \, .
  \end{equation*}
\end{definition}
\begin{remark}
  Recall two events $A, B$ are independent if $\P(A \given B) = \P(A)$, or equivalently $A, B$ satisfy the multiplication law: $\P(A \cap B) = \P(A) \P(B)$.
  A collection of events $A_1, \dots, A_N$ is said to be independent if \emph{any} sub collection $\set{A_{i_1}, \dots, A_{i_k}}$ satisfies the multiplication law.
  \emph{This is a stronger condition than simply requiring $\P(A_1 \cap \cdots \cap A_N) = \P(A_1) \cdots \P(A_N)$.}
  You should check, however, that if the random variables $X_1$, \dots, $X_N$, are all independent, then any collection of events of the form $\set{A_1, \dots A_N}$ with $A_i \in \sigma(X_i)$ is also independent.
\end{remark}

\begin{proposition}
  Let $X_1$, \dots, $X_N$ be $N$ random variables.
  The following are equivalent:
  \begin{enumerate}
    \item The random variables $X_1$, \dots, $X_N$ are independent.
    \item For every $\alpha_1, \dots, \alpha_N \in \R$, we have
      \begin{equation*}
	\P\paren[\Big]{
	  \bigcap_{j=1}^N \set{X_j \leq \alpha_j}
	}
	= \prod_{j=1}^N \P(X_j \leq \alpha_j)
      \end{equation*}
    \item For every collection of bounded continuous functions $f_1, \dots, f_N$ we have
      \begin{equation*}
	\E \brak[\Big]{ \prod_{j = 1}^N f_j(X_j) } = \prod_{j=1}^N \E f_j(X_j) \,.
      \end{equation*}

    \item
      For every $\xi_1, \dots, \xi_N \in \R$ we have
      \begin{equation*}
	\E \exp\paren[\Big]{ i\sum_{j=1}^{N} \xi_j X_j }
	= \prod_{j=1}^{N} \E \exp( i \xi_j X_j )\,,
	\quad\text{ where } i = \sqrt{-1}\,.
      \end{equation*}
  \end{enumerate}
\end{proposition}

\begin{remark}
  It is instructive to explicitly check each of these implications when $N = 2$ and $X_1, X_2$ are simple random variables.
\end{remark}

\begin{remark}
  The intuition behind the above result is as follows:
  Since the events $\set{X_j \leq \alpha_j}$ generate $\sigma(X_j)$, we expect the first two properties to be equivalent.
  Since $\one_{(-\infty, \alpha_j]}$ can be well approximated by continuous functions, we expect equivalence of the second and third properties.
  The last property is a bit more subtle: Since $\exp(a + b) = \exp(a) \exp(b)$, the third clearly implies the last property.
  The converse holds because of ``completeness of the complex exponentials'' or Fourier inversion, and again a through discussion of this will lead us too far astray.
\end{remark}

\begin{remark}
  The third implication above implies that independent random variables are uncorrelated. The converse, is of course false.
  However, the normal correlation theorem guarantees that jointly normal uncorrelated variables are independent.
\end{remark}


\subsection{The covariance of Brownian motion}

The independence of increments allows us to compute covariances of Brownian motion easily.
Suppose $W$ is a standard Brownian motion, and $s < t$.
Then we know $W_s \sim N(0, s)$, $W_t - W_s \sim N(0, t - s)$ and is independent of $W_s$.
Consequently $(W_s, W_t - W_s)$ is jointly normal with mean $0$ and covariance matrix $(\begin{smallmatrix} s & 0\\ 0 & t-s \end{smallmatrix})$.
This implies that $(W_s, W_t)$ is a jointly normal random variable.
Moreover we can compute the covariance by
\begin{equation*}
  \E W_s W_t  = \E W_s (W_t - W_s) + \E W_s^2 = s\,.
\end{equation*}
In general if you don't assume $s < t$, the above immediately implies $\E W_s W_t = s \varmin t$.

\section{The Martingale Property}

A martingale is ``fair game''.
Suppose you are playing a game and $M(t)$ is your cash stockpile at time $t$.
As time progresses, you learn more and more information about the game.
For instance, in blackjack getting a high card benefits the player more than the dealer, and a common card counting strategy is to have a ``spotter'' betting the minimum while counting the high cards.
When the odds of getting a high card are favorable enough, the player will signal a ``big player'' who joins the table and makes large bets, as long as the high card count is favorable.
Variants of this strategy have been shown to give the player up to a 2\% edge over the house.

If a game is a martingale, then this extra information you have acquired \emph{can not} help you going forward. 
That is, if you signal your ``big player'' at any point, you will not affect your expected return.

Mathematically this translates to saying that the \emph{conditional expectation} of your stockpile at a later time given your present accumulated knowledge, is exactly the present value of your stockpile.
Our aim in this section is to make this precise.

\subsection{Conditional probability}

Suppose you have an incomplete deck of cards which has $10$ red cards, and $20$ black cards.
Suppose $5$ of the red cards are \emph{high cards} (i.e.\ ace, king, queen, jack or $10$), and only $4$ of the black cards are high.
If a card is chosen at random, the \emph{conditional probability} of it being high given that it is red is $1/2$, and the \emph{conditional probability} of it being high given that it is black is $1/5$.
Our aim is to encode both these facts into a single entity.

We do this as follows.
Let $R, B$ denote the set of all red and black cards respectively, and $H$ denote the set of all high cards.
A $\sigma$-algebra encompassing all the above information is exactly
\begin{multline*}
  \mathcal G \defeq
    \bigl\{\emptyset, R, B, H, H^c, R\cap H, B\cap H, R\cap H^c, B\cap H^c,
      \\
      (R\cap H) \cup (B\cap H^c), (R\cap H^c) \cup (B\cap H), \Omega \bigr\}\,
\end{multline*}
and you can explicitly compute the probabilities of each of the above events.
A $\sigma$-algebra encompassing only the color of cards is exactly
\begin{equation*}
  \mathcal G \defeq \set{ \emptyset, R, B, \Omega }\,.
\end{equation*}

Now we define the \emph{conditional probability} of a card being high given the color to be the \textbf{random variable}
\begin{equation*}
  \P( H \given \mathcal C )
    \defeq \P(H \given R) \one_R + \P(H \given B) \one_B
    = \frac{1}{2} \one_R + \frac{1}{5} \one_B\,.
\end{equation*}
To emphasize:
\begin{enumerate}
  \item
    What is given is the $\sigma$-algebra $\mathcal C$, and not just an event.
  \item
    The conditional probability is now a \emph{$\mathcal C$-measurable random variable} and not a number.
\end{enumerate}

To see how this relates to $\P(H \given R)$ and $\P(H \given B)$ we observe
\begin{equation*}
  \int_R \P( H \given \mathcal C ) \, d\P
  \defeq \E \paren[\big]{ \one_R \P\paren{ H \given \mathcal C } }
  = \P( H \given R) \, \P(R) \,.
\end{equation*}
The same calculation also works for $B$, and so we have
\begin{equation*}
  \P( H \given R )
    = \frac{1}{\P(R)} \int_R \P( H \given \mathcal C ) \, d\P
  \quad\text{and}\quad
  \P( H \given B )
    = \frac{1}{\P(B)} \int_B \P\paren{ H \given \mathcal C } \, d\P\,.
\end{equation*}

Our aim is now to generalize this to a non-discrete scenario.
The problem with the above identities is that if either $R$ or $B$ had probability~$0$, then the above would become meaningless.
However, clearing out denominators yields
\begin{equation*}
  \int_R \P( H \given \mathcal C) \, d\P = \P( H \cap R )
  \quad\text{and}\quad
  \int_B \P( H \given \mathcal C) \, d\P = \P( H \cap B )\,.
\end{equation*}
This suggests that the defining property of $\P( H \given \mathcal C)$ should be the identity
\begin{equation}\label{e:cProb1}
  \int_C \P( H \given \mathcal C) \, d\P = \P( H \cap C )
\end{equation}
for every event $C \in \mathcal C$.
Note $\mathcal C = \set{ \emptyset, R, B, \Omega }$ and we have only checked~\eqref{e:cProb1} for $C = R$ and $C = B$.
However, for $C = \emptyset$ and $C = \Omega$, \eqref{e:cProb1} is immediate.

\begin{definition}
  Let $(\Omega, \mathcal G, \P)$ be a probability space, and $\mathcal F \subseteq \mathcal G$ be a $\sigma$-algebra.
  Given $A \in \mathcal G$, we define the conditional probability of $A$ given $\mathcal F$, denoted by $\P( A \given \mathcal F)$ to be an $\mathcal F$-measurable random variable that satisfies 
  \begin{equation}\label{e:cProb}
    \int_F \P( H \given \mathcal F) \, d\P = \P( H \cap F )
    \quad\text{ for every } F \in \mathcal F.
  \end{equation}
\end{definition}

\begin{remark}
  Showing existence (and uniqueness) of the conditional probability isn't easy, and relies on the \emph{Radon-Nikodym theorem}, which is beyond the scope of this course.
\end{remark}

\begin{remark}
  It is crucial to require that $\P( H \given \mathcal F )$ is measurable with respect to $\mathcal F$.
  Without this requirement we could simply choose $\P( H \given \mathcal F) = \one_H$ and~\eqref{e:cProb} would be satisfied.
  However, note that if $H \in \mathcal F$, then the function $\one_F$ is $\mathcal F$-measurable, and in this case $\P( H \given \mathcal F ) = \one_F$.
\end{remark}

\begin{remark}
  In general we can only expect~\eqref{e:cProb} to hold for all events in $\mathcal F$, and it need not hold for events in $\mathcal G$!
  Indeed, in the example above we see that
  \begin{equation*}
    \int_H \P( H \given \mathcal C) \, d\P
      = \frac{1}{2} \P(R \cap H) + \frac{1}{5} \P( B \cap H )
      = \frac{1}{2} \cdot \frac{5}{30} + \frac{1}{5} \cdot \frac{4}{30}
      = \frac{11}{100}
  \end{equation*}
  but
  \begin{equation*}
      \P(H \cap H) = \P(H) = \frac{3}{10} \neq \frac{11}{100}\,.
  \end{equation*}
\end{remark}

\begin{remark}
  One situation where you can compute $\P( A \given \mathcal F)$ explicitly is when $\mathcal F = \sigma( \set{F_i} )$ where $\set{F_i}$ is a pairwise disjoint collection of events whose union is all of $\Omega$ and $\P(F_i) > 0 $ for all $i$.
  In this case
  \begin{equation*}
    \P(A \given \mathcal F)
      = \sum_i \frac{\P( A \cap F_i )}{\P(F_i)} \one_{F_i} \,.
  \end{equation*}
\end{remark}

\subsection{Conditional expectation.}

Consider now the situation where $X$ is a $\mathcal G$-measurable random variable and $\mathcal F \subseteq \mathcal G$ is some $\sigma$-sub-algebra.
The conditional expectation of $X$ given $\mathcal F$ (denoted by $\E( X \given \mathcal F)$ is the ``best approximation'' of $X$ by a $\mathcal F$ measurable random variable.

Consider the incomplete deck example from the previous section, where you have an incomplete deck of cards which has $10$ red cards (of which $5$ are high), and $20$ black cards (of which $4$ are high).
Let $X$ be the outcome of a game played through a dealer who pays you $\$1$ when a high card is drawn, and charges you $\$1$ otherwise.
\emph{However, the dealer only tells you the color of the card drawn and your winnings, and not the rules of the game or whether the card was high.}

After playing this game often the only information you can deduce is that your expected return is $0$ when a red card is drawn and $-3/5$ when a black card is drawn.
That is, you approximate the game by the random variable
\begin{equation*}
  Y \defeq 0 \one_R - \frac{3}{5} \one_B\,,
\end{equation*}
where, as before $R, B$ denote the set of all red and black cards respectively. 

Note that the events you can deduce information about by playing this game (through the dealer) are exactly elements of the $\sigma$-algebra $\mathcal C = \set{\emptyset, R, B, \Omega}$.
By construction, that your approximation $Y$ is $\mathcal C$-measurable, and has the same averages as $X$ on all elements of $\mathcal C$.
That is, for every $C \in \mathcal C$, we have
\begin{equation*}
  \int_C Y \, d\P = \int_C X \, d\P \,.
\end{equation*}
This is how we define conditional expectation.

\begin{definition}
  Let $X$ be a $\mathcal G$-measurable random variable, and $\mathcal F \subseteq \mathcal G$ be a $\sigma$-sub-algebra.
  We define $\E(X \given \mathcal F)$, the \emph{conditional expectation of $X$ given $\mathcal F$} to be a \emph{random variable} such that:
  \begin{enumerate}
    \item
      $\E( X \given \mathcal F)$ is $\mathcal F$-measurable.

    \item
      For every $F \in \mathcal F$, we have the \emph{partial averaging} identity:
      \begin{equation}\label{e:2pavg}
	\int_F \E(X \given \mathcal F) \, d\P = \int_F X \, d\P.
      \end{equation}
  \end{enumerate}
\end{definition}

\begin{remark}
  Choosing $F = \Omega$ we see $\E \E(X\given \mathcal F) = \E X$.
\end{remark}
\begin{remark}
  Note we can only expect~\eqref{e:2pavg} to hold for all events $F \in \mathcal F$.
  In general~\eqref{e:2pavg} \emph{will not} hold for events $G \in \mathcal G - \mathcal F$.
\end{remark}
\begin{remark}
  Under mild integrability assumptions one can show that conditional expectations exist.
  This requires the \emph{Radon-Nikodym} theorem and goes beyond the scope of this course.
  If, however, $\mathcal F = \sigma( \set{F_i} )$ where $\set{F_i}$ is a pairwise disjoint collection of events whose union is all of $\Omega$ and $\P(F_i) > 0 $ for all $i$, then
  \begin{equation*}
    \E( X \given \mathcal F)
      = \sum_{i=1}^{\infty}  \frac{\one_{F_i}}{\P(F_i)} \int_{F_i} X \, d\P \,.
  \end{equation*}
\end{remark}
\begin{remark}
  Once existence is established it is easy to see that conditional expectations are unique.
  Namely, if $Y$ is any $\mathcal F$-measurable random variable that satisfies
  \begin{equation*}
    \int_F Y \, d\P = \int_F X \, d\P
    \quad\text{for every } F \in \mathcal F\,,
  \end{equation*}
  then $Y = \E( X \given F)$.
  Often, when computing the conditional expectation, we will ``guess'' what it is, and verify our guess by checking measurablity and the above partial averaging identity. 
\end{remark}
\begin{proposition}
If $X$ is $\mathcal F$-measurable, then $\E(X \given \mathcal F) = X$.
On the other hand, if $X$ is independent\footnote{%
  We say a random variable $X$ is independent of $\sigma$-algebra $\mathcal F$ if for every $A \in \sigma(X)$ and $B \in \mathcal F$ the events $A$ and $B$ are independent.%
}
of~$\mathcal F$ then $\E( X \given \mathcal F) = \E X$.
\end{proposition}
\begin{proof}
  If $X$ is $\mathcal F$-measurable, then clearly the random variable $X$ is both $\mathcal F$-measurable and satisfies the partial averaging identity.
  Thus by uniqueness, we must have $\E( X \given \mathcal F) = X$.
  
  Now consider the case when $X$ is independent of $\mathcal F$.
  Suppose first $X = \sum a_i \one_{A_i}$ for finitely many sets $A_i \in \mathcal G$.
  Then for any $F \in \mathcal F$,
  \begin{equation*}
    \int_F X \, d\P
      = \sum a_i \P( A_i \cap F )
      = \P(F) \sum a_i \P(A_i)
      = \P(F) \E X
      = \int_F \E X \, d\P\,.
  \end{equation*}
  Thus the constant random variable $\E X$ is clearly $\mathcal F$-measurable and satisfies the partial averaging identity.
  This forces $\E( X \given \mathcal F) = \E X$.
  The general case when $X$ is not simple follows by approximation.
\end{proof}

The above fact has a generalization that is tremendously useful when computing conditional expectations.
Intuitively, the general principle is to \emph{average} quantities that are independent of $\mathcal F$, and \emph{leave unchanged} quantities that are $\mathcal F$ measurable.
This is known as the independence lemma.
\begin{lemma}[Independence Lemma]
  Suppose $X, Y$ are two random variables such that $X$ is independent of $\mathcal F$ and $Y$ is $\mathcal F$-measurable.
  Then if $f = f(x, y)$ is any function of two variables we have
  \begin{equation*}
    \E \paren[\big]{ f(X, Y) \given \mathcal F}
      = g(Y)\,,
  \end{equation*}
  where $g = g(y)$ is the function%
  \footnote{%
    To clarify, we are defining a function $g = g(y)$ here when $y \in \R$ is any real number.
    Then, once we compute $g$, we substitute in $y = Y (= Y(\omega))$, where $Y$ is the given random variable.%
  }
  defined by
  \begin{equation*}
    g(y) \defeq \E f(X, y)\,.
  \end{equation*}
\end{lemma}

\begin{remark*}
  If $p_X$ is the probability density function of $X$, then the above says
  \begin{equation*}
    \E\paren[\big]{ f(X, Y) \given \mathcal F}
      = \int_\R f(x, Y) \, p_X(x) \, dx\,.
  \end{equation*}
  Indicating the $\omega$ dependence explicitly for clarity, the above says
  \begin{equation*}
    \E\paren[\big]{ f(X, Y) \given \mathcal F}(\omega)
      = \int_\R f(x, Y(\omega)) \, p_X(x) \, dx\,.
  \end{equation*}
\end{remark*}
\iffalse
\begin{remark}
  In Lebesgue integral notation (with the $\omega$ dependence explicitly indicated) this simply says that
  \begin{equation*}
    \E( f(X, Y) \given \mathcal F )(\omega)
      = \int_\Omega f( X(\xi), Y(\omega) ) \, d\P(\xi)\,.
  \end{equation*}
\end{remark}
\fi


\begin{remark}
  Note we defined and motivated conditional expectations and conditional probabilities independently.
  They are however intrinsically related:
  Indeed, $\E( \one_A \given \mathcal F) = \P( A \given \mathcal F)$, and this can be checked directly from the definition.
\end{remark}

Conditional expectations are tremendously important in probability, and we will encounter it often as we progress.
This is probably the first visible advantage of the measure theoretic approach, over the previous ``intuitive'' or discrete approach to probability.

\begin{proposition}
  Conditional expectations satisfy the following properties.
  \begin{enumerate}
    \item
      \emph{(Linearity)}
      If $X, Y$ are random variables, and $\alpha \in \R$ then
      \begin{equation*}
	\E( X + \alpha Y \given \mathcal F ) = \E( X \given \mathcal F ) + \alpha \E( Y \given \mathcal F )\,.
      \end{equation*}

    \item
      \emph{(Positivity)}
      If $X \leq Y$, then $\E(X \given \mathcal F) \leq \E(Y \given \mathcal F)$ (almost surely).

    \item If $X$ is $\mathcal F$ measurable and $Y$ is an arbitrary (not necessarily $\mathcal F$-measurable) random variable then (almost surely)
      \begin{equation*}
	\E( XY \given \mathcal F) = X \E( Y \given \mathcal F ) \,.
      \end{equation*}
    \item
      \emph{(Tower property)}
      If $\mathcal E \subseteq \mathcal F \subseteq \mathcal G$ are $\sigma$-algebras, then (almost surely)
      \begin{equation*}
	\E\paren{ X \given \mathcal E } =
	  \E\paren[\Big]{ \E\paren{ X \given \mathcal F } \given \mathcal E }\,.
      \end{equation*}
  \end{enumerate}
\end{proposition}
\begin{proof}
  The first property follows immediately from linearity.
  For the second property, set $Z = Y - X$ and observe
  \begin{equation*}
    \int_{\E(Z \given \mathcal F)} \E(Z \given \mathcal F) \, d\P
    = \int_{\E(Z \given \mathcal F)} Z \, d\P
    \geq 0\,,
  \end{equation*}
  which can only happen if $\P( \E( Z \given \mathcal F ) < 0 ) = 0$.
  The third property is easily checked for simple random variables, and follows in general by approximating.
  The tower property follows immediately from the definition.
\end{proof}



\subsection{Adapted processes and filtrations}

Let $X$ be any stochastic process (for example Brownian motion).
For any $t > 0$, we've seen before that $\sigma(X(t))$ represents the information you obtain by observing $X(t)$.
Accumulating this over time gives us the \emph{filtration}.

\begin{definition}
  The \emph{filtration generated $X$} is the family of $\sigma$-algebras $\set{\mathcal F^X_t \st t \geq 0}$ where
  \begin{equation*}
    \mathcal F_t^X \defeq \sigma\paren[\Big]{ \bigcup_{s \leq t} \sigma(X_s) }
    \,.
  \end{equation*}
\end{definition}

Clearly each $\mathcal F_t^X$ is a $\sigma$-algebra, and if $s \leq t$, $\mathcal F^X_s \subseteq \mathcal F^X_t$.
A family of $\sigma$-algebras with this property is called a \emph{filtration}.

\begin{definition}
  A \emph{filtration} is a family of $\sigma$-algebras $\set{ \mathcal F_t \st t \geq 0}$ such that whenever $s \leq t$, we have $\mathcal F_s \subseteq \mathcal F_t$.
\end{definition}

The $\sigma$-algebra $\mathcal F_t$ represents the information accumulated up to time $t$.
When given a filtration, it is important that all stochastic processes we construct respect the flow of information because trading / pricing strategies can not rely on the price at a later time, and gambling strategies do not know the outcome of the next hand.
This is called \emph{adapted}.

\begin{definition}
  A stochastic process $X$ is said to be \emph{adapted} to a filtration $\set{\mathcal F_t \st t \geq 0}$ if for every $t$ the random variable $X(t)$ is $\mathcal F_t$ measurable (i.e.\ $\set{X(t) \leq \alpha} \in \mathcal F_t$ for every $\alpha \in \R$, $t \geq 0$).
\end{definition}

Clearly a process $X$ is adapted with respect to the filtration it generates $\set{\mathcal F_t^X}$.
%Most of the time the filtration chosen will be the \emph{Brownian filtrations} (i.e.\ the filtration generated by Brownian motion).

\subsection{Martingales}

Recall, a martingale is a ``fair game''.
Using conditional expectations, we can now define this precisely.

\begin{definition}
  A stochastic process $M$ is a martingale with respect to a filtration $\set{\mathcal F_t}$ if:
  \begin{enumerate}
    \item
       $M$ is adapted to the filtration $\set{\mathcal F_t}$.
    \item
      For any $s < t$ we have $\E\paren{ M(t) \given \mathcal F_s} = M(s)$, almost surely.
  \end{enumerate}
\end{definition}
\begin{remark}
  A \emph{sub-martingale} is an adapted process $M$ for which we have $\E(M(t) \given \mathcal F_s) \geq M(s)$, and a \emph{super-martingale} if $\E( M(t) \given \mathcal F_s) \leq M(s)$.
  Thus $\E M(t)$ is an increasing function of time if $M$ is a sub-martingale, constant in time if $M$ is a martingale, and a decreasing function of time if $M$ is a super-martingale.
\end{remark}

\begin{remark}
  It is crucial to specify the filtration when talking about martingales, as it is certainly possible that a process is a martingale with respect to one filtration but not with respected to another.
  For our purposes the filtration will almost always be the \emph{Brownian filtration} (i.e.\ the filtration generated by Brownian motion).
\end{remark}

\begin{example}
  Let $\set{\mathcal F_t}$ be a filtration, $\mathcal F_\infty = \sigma( \cup_{t \geq 0} \mathcal F_t )$, and $X$ be any $\mathcal F_\infty$-measurable random variable.
  The process $M(t) \defeq \E\paren{ X_\infty \given \mathcal F_t }$ is a martingale with respect to the filtration $\set{\mathcal F_t}$.
\end{example}


In discrete time a random walk is a martingale, so it is natural to expect that in continuous time Brownian motion is a martingale as well.

\begin{theorem}
  Let $W$ be a Brownian motion, $\mathcal F_t = \mathcal F_t^W$ be the Brownian filtration.
  Brownian motion is a martingale with respect to this filtration.
\end{theorem}
\begin{proof}
  By independence of increments, $W(t) - W(s)$ is certainly independent of $W(r)$ for any $r \leq s$.
  Since $\mathcal F_s = \sigma( \cup_{r \leq s}\sigma(W(r)) )$ we expect that $W(t) - W(s)$ is independent of~$\mathcal F_s$.
  Consequently
  \begin{equation*}
    \E( W(t) \given \mathcal F_s )
      = \E( W(t) - W(s) \given \mathcal F_s)
        + \E( W(s) \given \mathcal F_s)
      = 0 + W(s) = W(s)\,.
      \qedhere
  \end{equation*}
\end{proof}

\begin{theorem}
  Let $W$ be a standard Brownian motion (i.e. a Brownian motion normalized so that $W(0) = 0$ and $\var(W(t)) = t$).
  For any $C^{1,2}_b$ function%
  \footnote{%
    Recall a function $f = f(t, x)$ is said to be $C^{1,2}$ if it is $C^1$ in $t$ (i.e.\ differentiable with respect to $t$ and $\partial_t f$ is continuous), and $C^2$ in $x$ (i.e.\ twice differentiable with respect to $x$ and $\partial_x f$, $\partial_x^2 f$ are both continuous).
    The space $C^{1,2}_b$ refers to all $C^{1,2}$ functions $f$ for which and $f$, $\partial_t f$, $\partial_x f$, $\partial_x^2 f$ are all bounded functions.%
  }
  $f = f(t, x)$ the process
  \begin{equation*}
    M(t) \defeq f(t, W(t))
      - \int_0^t \paren[\Big]{
	  \partial_t f(s, W(s)) + \frac{1}{2} \partial_x^2 f(s, W(s))
	} \, ds
  \end{equation*}
  is a martingale (with respect to the Brownian filtration).
\end{theorem}
\begin{proof}
  This is an extremely useful fact about Brownian motion follows quickly from the It\^o formula, which we will discuss later.
  However, at this stage, we can provide a simple, elegant and instructive proof as follows.

  Adaptedness of $M$ is easily checked.
  To compute $\E( M(t) \given \mathcal F_r )$ we first observe
  \begin{equation*}
    \E \paren[\big]{ f( t, W(t) ) \given \mathcal F_r }
    = \E \paren[\big]{ f( t, \brak{W(t) - W(r)} + W(r) ) \given \mathcal F_r }.
  \end{equation*}
  Since $W(t) - W(r) \sim N(0, t - r)$ and is independent of $\mathcal F_r$, the above conditional expectation can be computed by
  \begin{equation*}
    \E \paren[\big]{ f( t, \brak{W(t) - W(r)} + W(r) ) \given \mathcal F_r }
    = \int_\R f(t, y + W(r)) G(t -r, y) \, dy \,,
  \end{equation*}
  where
  \begin{equation*}
    G(\tau , y ) = \frac{1}{\sqrt{2 \pi \tau}} \exp\paren[\Big]{ \frac{-y^2}{\tau} }
  \end{equation*}
  is the density of $W(t) - W(r)$.

  Similarly
  \begin{align*}
    \MoveEqLeft
    \E\paren[\Big]{
      \int_0^t \paren[\big]{
	\partial_t f(s, W(s)) + \frac{1}{2} \partial_x^2 f(s, W(s))
      } \, ds \given \mathcal F_r}
    \\
    &= \int_0^r \paren[\big]{ \partial_t f(s, W(s)) + \frac{1}{2} \partial_x^2 f(s, W(s)) } \, ds
    \\
    &\qquad
      + \int_r^t \int_\R \paren[\big]{
	  \partial_t f(s, y + W(r)) + \frac{1}{2} \partial_x^2 f(s, y + W(r))
	} G(s - r, y) \, ds
  \end{align*}
  Hence
  \begin{align*}
    \MoveEqLeft
    \E\paren{ M(t) \given \mathcal F_r } - M(r)
    = \int_\R f(t, y + W(r)) G(t -r, y) \, dy 
    \\
      &- \int_r^t \int_\R \paren[\big]{
	  \partial_t f(s, y + W(r)) + \frac{1}{2} \partial_x^2 f(s, y + W(r))
	} G(s - r, y) \, ds
    \\
      &- f(r, W(r))\,.
  \end{align*}
  We claim that the right hand side of the above vanishes.
  In fact, we claim the (deterministic) identity 
  \begin{multline*}
    \int_r^t \int_\R \paren[\big]{
	\partial_t f(s, y + x) + \frac{1}{2} \partial_x^2 f(s, y + x))
      } G(s - r, y) \, ds
    \\
    + \int_\R f(t, y + x) G(t -r, y) \, dy 
      - f(r, x) = 0 \,,
  \end{multline*}
  holds for any function $f$ and $x \in \R$.
  For those readers who are familiar with PDEs, this is simply the Duhamel's principle for the heat equation.
  If you're unfamiliar with this, the above identity can be easily checked using the fact that $\partial_\tau G = \frac{1}{2} \partial_y^2 G$ and integrating the first integral by parts.
  We leave this calculation to the reader.
\end{proof}
\subsection{Stopping Times}

For this section we assume that a filtration $\set{\mathcal F_t}$ is given to us, and fixed.
When we refer to process being adapted (or martingales), we implicitly mean they are adapted (or martingales) with respect to this filtration.

Consider a game (played in continuous time) where you have the option to walk away at any time.
Let $\tau$ be the \emph{random} time you decide to stop playing and walk away.
In order to respect the flow of information, you need to be able to decide weather you have stopped using only information up to the present.
At time $t$, event $\set{\tau \leq t}$ is exactly when you have stopped and walked away.
Thus, to respect the flow of information, we need to ensure $\set{\tau \leq t} \in \mathcal F_t$.

\begin{definition}
  A stopping time is a function $\tau\colon \Omega \to [0, \infty)$ such that for every $t \geq 0$ the event $\set{\tau \leq t} \in \mathcal F_t$.
\end{definition}

A standard example of a stopping time is \emph{hitting times}.
Say you decide to liquidate your position once the value of your portfolio reaches a certain threshold.
The time at which you liquidate is a hitting time, and under mild assumptions on the filtration, will always be a stopping time.

\begin{proposition}
  Let $X$ be an adapted continuous process, $\alpha \in \R$ and $\tau$ be the first time $X$ hits $\alpha$ (i.e.\ $\tau  = \inf\set{t \geq 0 \st X(t) = \alpha })$.
  Then $\tau$ is a stopping time (if the filtration is right continuous).
\end{proposition}

\begin{theorem}[Doob's optional sampling theorem]
  If $M$ is a martingale and $\tau$ is a bounded stopping time.
  Then the stopped process $M^\tau(t) \defeq M(\tau \varmin t)$ is also a martingale.
  Consequently, $\E M(\tau) = \E M(\tau \varmin t) = \E M(0) = \E M(t)$ for all $t \geq 0$.
\end{theorem}
\begin{remark}
  If instead of assuming $\tau$ is bounded, we assume $M^\tau$ is bounded the above result is still true.
\end{remark}

The proof goes beyond the scope of these notes, and can be found in any standard reference.
What this means is that if you're playing a fair game, then you can not hope to improve your odds by ``quitting when you're ahead''.
Any rule by which you decide to stop, must be a stopping time and the above result guarantees that stopping a martingale still yields a martingale.

\begin{remark}
  Let $W$ be a standard Brownian motion, $\tau$ be the first hitting time of $W$ to $1$.
  Then $\E W(\tau) = 1 \neq 0 = \E W(t)$.
  This is one situation where the optional sampling theorem doesn't apply (in fact, $\E \tau = \infty$, and $W^\tau$ is unbounded).
  
  This example corresponds to the gambling strategy of walking away when you make your ``million''.
  The reason it's not a sure bet is because the time taken to achieve your winnings is finite almost surely, but very long (since $\E \tau = \infty$).
  In the mean time you might have incurred financial ruin and expended your entire fortune.
\end{remark}

Suppose the price of a security you're invested in fluctuates like a martingale (say for instance Brownian motion).
This is of course unrealistic, since Brownian motion can also become negative; but lets use this as a first example.
You decide you're going to liquidate your position and walk away when either you're bankrupt, or you make your first million.
What are your expected winnings?
This can be computed using the optional sampling theorem.

\begin{problem}
  Let $a \geq 0$ and $M$ be any continuous martingale with $M(0) = x \in (0, a)$.
  Let $\tau$ be the first time $M$ hits either $0$ or $a$.
  Compute $\P( M(\tau) = a )$ and your expected return $\E M(\tau)$.
\end{problem}

\end{document}
