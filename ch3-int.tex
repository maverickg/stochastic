%&subfile
\begin{document}
\ifstandalonechapter\setcounter{chapter}{2}\fi
\chapter{Stochastic Integration}

\section{Motivation}
Suppose $\Delta(t)$ is your position at time $t$ on a security whose price is $S(t)$.
If you only trade this security at times $0 = t_0 < t_1 < t_2 < \cdots < t_n = T$, then the value of your winnings up to time $T$ is exactly
\begin{equation*}
  X(t_n) = \sum_{i=0}^{n-1} \Delta(t_i) ( S(t_{i+1}) - S(t_i) )
\end{equation*}
If you are trading this continuously in time, you'd expect that a ``simple'' limiting procedure should show that your winnings are given by the \emph{Riemann-Stieltjes} integral:
\begin{equation*}%\label{e:3stLim}
  X(T) = 
    \lim_{\norm{P} \to 0}
      \sum_{i=0}^{n-1} \Delta(t_i) \paren{ S( t_{i+1} ) - S( t_i ) }
    = \int_0^T \Delta(t) \, dS(t) \,.
\end{equation*}
Here $P = \set{0 = t_0 < \cdots < t_n = T}$ is a partition of $[0, T]$, and $\norm{P} = \max\set{ t_{i+1} - t_i }$.

%The reason we have $X(\xi_i)$ above, and not $X(t_i)$ is because the standard definition of the \emph{Riemann-Stieltjes} allows you to choose \emph{arbitrary} times in the partition sub-intervals $[t_i, t_{i+1}]$.
%In our context, this doesn't sit right: If you trade at time $t_i$, you should do so using only information available to you up to time $t_i$ (i.e.\ be $\mathcal F_{t_i}$-measurable), and your winnings are $X(t_i) (S(t_{i+1}) - S(t_i) )$ and not $X(\xi_i) (S(t_{i+1}) - S(t_i) )$.
%Indeed, it turns out that if $S$ is a continuous martingale, then the limit of the form~\eqref{e:3stLim} \emph{will not} exist in general.

This has been well studied by mathematicians, and it is well known that for the above limiting procedure to ``work directly'', you need $S$ to have finite \emph{first variation}.
Recall, the \emph{first variation} of a function is defined to be
\begin{equation*}
  V_{[0,T]}(S) \defeq \lim_{\norm{P} \to 0} \sum_{i=0}^{n-1} \abs{S(t_{i+1}) - S(t_i)}\,.
\end{equation*}
It turns out that almost \emph{any} continuous martingale~$S$ \emph{will not} have finite first variation.
Thus to define integrals with respect to martingales, one has to do something `clever'.
It turns out that if $X$ is adapted and $S$ is an martingale, then the above limiting procedure works, and this was carried out by It\^o (and independently by Doeblin).

\section{The First Variation of Brownian motion}

We begin by showing that the first variation of Brownian motion is infinite.
\begin{proposition}
  If $W$ is a standard Brownian motion, and $T > 0$ then
  \begin{equation*}
    \lim_{n \to \infty} \E \sum_{k=0}^{n-1}
      \abs[\Big]{
	W\paren[\Big]{\frac{k+1}{n}}
	- W\paren[\Big]{\frac{k}{n}}
      } = \infty\,.
  \end{equation*}
\end{proposition}
\begin{remark}
  In fact
  \begin{equation*}
    \lim_{n \to \infty} \sum_{k=0}^{n-1}
      \abs[\Big]{
	W\paren[\Big]{\frac{k+1}{n}}
	- W\paren[\Big]{\frac{k}{n}}
      } = \infty
      \quad\text{almost surely,}
  \end{equation*}
  but this won't be necessary for our purposes.
\end{remark}
\begin{proof}
  Since $W( (k+1)/n ) - W(k/n) \sim N(0, 1/n)$ we know
  \begin{equation*}
    \E \abs[\Big]{
	W\paren[\Big]{\frac{k+1}{n}}
	- W\paren[\Big]{\frac{k}{n}}
      }
      = \int_\R \abs{x} \, G\paren[\Big]{ \frac{1}{n}, x } \, dx
      = \frac{C}{\sqrt{n}}\,,
  \end{equation*}
  where
  \begin{equation*}
     C = \int_\R \abs{y} e^{-y^2/2} \, \frac{dy}{\sqrt{2\pi}}
      = \E \abs{N(0, 1)} \,.
  \end{equation*}
  Consequently
  \begin{equation*}
    \sum_{k=0}^{n-1}
      \E
      \abs[\Big]{
	W\paren[\Big]{\frac{k+1}{n}}
	- W\paren[\Big]{\frac{k}{n}}
      }
      = \frac{C n}{\sqrt{n}}
      \xrightarrow{n \to \infty} \infty\,.
      \qedhere
  \end{equation*}
\end{proof}

\section{Quadratic Variation}

It turns out that the \emph{second} variation of any \emph{square integrable} martingale is almost surely finite, and this is the key step in constructing the It\^o integral.
\begin{definition}
  Let $M$ be any process.
  We define the \emph{quadratic variation} of $M$, denoted by $\qv{M}$ by
  \begin{equation*}
    \qv{M}(T) = \lim_{\norm{P} \to 0} (M(t_{i+1}) - M(t_i))^2\,,
  \end{equation*}
  where $P = \set{0 = t_1 < t_1 \cdots < t_n = T}$ is a partition of $[0, T]$.
\end{definition}

\begin{proposition}
  If $W$ is a standard Brownian motion, then $\qv{W}(T) = T$ almost surely.
\end{proposition}
\begin{proof}
  For simplicity, let's assume $t_i = Ti/n$.
  Note
  \begin{equation*}
    \sum_{i=0}^{n-1}
      \paren[\Big]{
	W\paren[\Big]{\frac{(i+1)T}{n}}
	- W\paren[\Big]{\frac{iT}{n}}
      }^2 - T
      = \sum_{i=0}^{n-1}
	  \xi_i\,,
  \end{equation*}
  where
  \begin{equation*}
    \xi_i \defeq
	    \paren[\Big]{
	      W\paren[\Big]{\frac{(i+1)T}{n}}
	      - W\paren[\Big]{\frac{iT}{n}}
	    }^2 - \frac{T}{n}\,.
  \end{equation*}
  Note that $\xi_i$'s are i.i.d.\ with distribution $N(0, T/n)^2 - T/n$, and hence
  \begin{equation*}
    \E \xi_i = 0
    \qquad\text{and}\qquad
    \var \xi_i = \frac{T^2 (\E N(0,1)^4 - 1)}{n^2}.
  \end{equation*}
  Consequently
  \begin{equation*}
    \var\paren[\Big]{
      \sum_{i=0}^{n-1} \xi_i
    } = \frac{T^2 (\E N(0,1)^4 - 1)}{n} \xrightarrow{n \to \infty} 0\,,
  \end{equation*}
  which shows
  \begin{equation*}
    \sum_{i=0}^{n-1}
      \paren[\Big]{
	W\paren[\Big]{\frac{(i+1)T}{n}}
	- W\paren[\Big]{\frac{iT}{n}}
      }^2 - T
      = \sum_{i=0}^{n-1}
	  \xi_i
      \xrightarrow{n \to \infty} 0 \,.
      \qedhere
  \end{equation*}
\end{proof}

\begin{corollary}
  The process $M(t) \defeq W(t)^2 - \qv{W}(t)$ is a martingale.
\end{corollary}
\begin{proof}
  We see
  \begin{multline*}
    \E( W(t)^2 - t \given \mathcal F_s )
      = \E ( (W(t) - W(s))^2  + 2 W(s) (W(t) - W(s)) + W(s)^2 \given \mathcal F_s ) - t
      \\
      = W(s)^2 - s\,
  \end{multline*}
  and hence $\E (M(t) \given \mathcal F_s ) = M(s)$.
\end{proof}

The above wasn't a co-incidence.
This property in fact characterizes the quadratic variation.
\begin{theorem}\label{t:3qv1}
  Let $M$ be a continuous martingale with respect to a filtration~$\set{\mathcal F_t}$.
  Then $\E M(t)^2 < \infty$ if and only if $\E \qv{M}(t) < \infty$.
  In this case the process $M(t)^2 - \qv{M}(t)$ is also a martingale with respect to the same filtration, and hence $\E M(t)^2 - \E M(0)^2 = \E \qv{M}(t)$.
\end{theorem}

The above is in fact a characterization of the quadratic variation of martingales.

\begin{theorem}\label{t:3qv2}
  If $A(t)$ is any continuous, increasing, adapted process such that $M(t)^2 - A(t)$ is a martingale, then $A = \qv{M}$.
\end{theorem}

The proof of these theorems are a bit technical and go beyond the scope of these notes.
The results themselves, however, are extremely important and will be used subsequently.

\begin{remark}
  The intuition to keep in mind about the first variation and the quadratic variation is the following.
  Divide the interval $[0, T]$ into $T/\delta t$ intervals of size $\delta t$.
  If $X$ has finite first variation, then on each subinterval $(k \delta t, (k+1) \delta t)$ the increment of $X$ should be of order $\delta t$.
  Thus adding $T / \delta t$ terms of order $\delta t$ will yield something finite.

  On the other hand if $X$ has finite quadratic variation, on each subinterval $(k \delta t, (k+1) \delta t)$ the increment of $X$ should be of order $\sqrt{\delta t}$, so that adding $T / \delta t$ terms of the \emph{square} of the increment yields something finite.
  Doing a quick check for Brownian motion (which has finite quadratic variation), we see
  \begin{equation*}
    \E \abs{W(t + \delta t) - W(t)} = \sqrt{\delta t} \E \abs{N(0, 1)}\,,
    %\quad\text{and}\quad
    %\E \abs{W(t + \delta t) - W(t)}^2 = \delta t\,,
  \end{equation*}
  which is in line with our intuition.
\end{remark}

\begin{remark}
  If a continuous process has finite first variation, its quadratic variation will necessarily be $0$.
  On the other hand, if a continuous process has finite (and non-zero) quadratic variation, its first variation will necessary be infinite.
\end{remark}


\section{Construction of the It\^o integral}

Let $W$ be a standard Brownian motion, $\set{\mathcal F_t}$ be the Brownian filtration and $\Delta$ be an adapted process.
We think of $\Delta(t)$ to represent our position at time $t$ on an asset whose spot price is $W(t)$.

\begin{lemma}\label{l:3dito}
Let $P = \set{0 = t_0 < t_1 < t_2 < \cdots}$ be an increasing sequence of times, and assume $\Delta$ is constant on $[t_i, t_{i+1})$ (i.e.\ the asset is only traded at times $t_0$, \dots, $t_n$).
Let $I_P(T)$, defined by
\begin{equation*}
  I_P(T)
    = \sum_{i=0}^{n-1} \Delta(t_i) (W(t_{i+1}) - W(t_i))
      + \Delta(t_n) (W(T) - W(t_n))
  \quad\text{if } T \in [t_n, t_{n+1})\,.
\end{equation*}
be your cumulative winnings up to time $T$.
Then,
\begin{equation}\label{e:3Eip2}
  \E I_P(T)^2
    = \E \brak[\Big]{ \sum_{i=0}^{n} \Delta(t_i)^2 (t_{i+1} - t_i)
      + \Delta(t_n)^2 (T - t_n) }
  \quad\text{if } T \in [t_n, t_{n+1})\,.
\end{equation}
Moreover, $I_P$ is a martingale and
\begin{equation}\label{e:3dqv}
  \qv{I_P}(T) = \sum_{i=0}^{n} \Delta(t_i)^2 (t_{i+1} - t_i)
      + \Delta(t_n)^2 (T - t_n)
  \quad\text{if } T \in [t_n, t_{n+1})\,.
\end{equation}
\end{lemma}

This lemma, as we will shortly see, is the key to the construction of stochastic integrals (called It\^o integrals).
\begin{proof}%[Proof of Lemma~\ref{l:3dito}]
  We first prove~\eqref{e:3Eip2} with $T = t_n$ for simplicity.
  Note
  \begin{multline}\label{e:3sum1}
    \E I_P(t_n)^2 
      = \sum_{i=0}^{n-1}
	  \E \Delta(t_i)^2 (W(t_{i+1}) - W(t_i))^2
    \\
	+ 2 \sum_{j=0}^{n-1}
	  \sum_{i=0}^{i-1}
	  \E \Delta(t_i) \Delta(t_j)
	    (W(t_{i+1}) - W(t_i))
	    (W(t_{j+1}) - W(t_j))
  \end{multline}
  By the tower property
  \begin{multline*}
    \E \Delta(t_i)^2 (W(t_{i+1}) - W(t_i))^2
      = \E \E \paren[\big]{ \Delta(t_i)^2 (W(t_{i+1}) - W(t_i))^2 \given \mathcal F_{t_i} }
    \\
      = \E \Delta(t_i)^2 \E \paren[\big]{ (W(t_{i+1}) - W(t_i))^2 \given \mathcal F_{t_i} }
      = \E \Delta(t_i)^2 (t_{i+1} - t_i)\,.
  \end{multline*}
  Similarly we compute
  \begin{align*}
    \MoveEqLeft
    \E \Delta(t_i) \Delta(t_j)
      (W(t_{i+1}) - W(t_i))
      (W(t_{j+1}) - W(t_j))
    \\
    &= \E \E \paren[\big]{ \Delta(t_i) \Delta(t_j)
      (W(t_{i+1}) - W(t_i))
      (W(t_{j+1}) - W(t_j)) \given \mathcal F_{t_j} }
    \\
    &= \E \Delta(t_i) \Delta(t_j)
	  (W(t_{i+1}) - W(t_i))
	  \E \paren[\big]{ (W(t_{j+1}) - W(t_j)) \given \mathcal F_{t_j} }
    = 0\,.
  \end{align*}
  Substituting these in~\eqref{e:3sum1} immediately yields~\eqref{e:3Eip2} for $t_n = T$.

  The proof that $I_P$ is an martingale uses the same ``tower property'' idea, and is left to the reader to check.
  The proof of~\eqref{e:3dqv} is also similar in spirit, but has a few more details to check.
  The main idea is to let $A(t)$ be the right hand side of~\eqref{e:3dqv}.
  Observe~$A$ is clearly a continuous, increasing, adapted process.
  Thus, if we show $M^2 - A$ is a martingale, then using Theorem~\ref{t:3qv2} we will have $A = \qv{M}$ as desired.
  The proof that $M^2 - A$ is an martingale uses the same ``tower property'' idea, but is a little more technical and is left to the reader.
\end{proof}

Note that as $\norm{P} \to 0$, the right hand side of~\eqref{e:3dqv} converges to the standard Riemann integral $\int_0^T \Delta(t)^2 \, dt$.
It\^o realised he could use this to prove that $I_P$ itself converges, and the limit is now called the It\^o integral.

\begin{theorem}\label{t:3ipconv}
  If $\int_0^T \Delta(t)^2 \, dt < \infty$ almost surely, then as $\norm{P} \to 0$, the processes $I_P$ converge to a \emph{continuous process} $I$ denoted by
  \begin{equation}\label{e:3Idef}
    I(T)
      \defeq \lim_{\norm{P} \to 0} I_P(T)\,
      \defeq \int_0^T \Delta(t) \, dW(t)\,.
  \end{equation}
  This is known as the \emph{It\^o integral of $\Delta$ with respect to $W$}.
  If further
  \begin{equation}\label{e:3Dsq}
    \E \int_0^T \Delta(t)^2 \, dt < \infty\,,
  \end{equation}
  then the process $I(T)$ is a \emph{martingale} and the quadratic variation $\qv{I}$ satisfies
  \begin{equation*}
    \qv{I}(T) = \int_0^T \Delta(t)^2 \, dt
    \quad\text{almost surely.}
  \end{equation*}
\end{theorem}

\begin{remark}
  For the above to work, it is \emph{crucial} that $\Delta$ is adapted, and is sampled at the left endpoint of the time intervals.
  That is, the terms in the sum are $\Delta(t_i) (W(t_{i+1}) - W(t_i))$, and not $\Delta(t_{i+1}) (W(t_{i+1}) - W(t_i))$ or $\frac{1}{2} (\Delta(t_i) + \Delta(t_{i+1})) (W(t_{i+1}) - W(t_i))$, or something else.

  Usually if the process is not adapted, there is no meaningful way to make sense of the limit.
  However, if you sample at different points, it still works out (usually) but what you get is \emph{different} from the It\^o integral (one example is the Stratonovich integral). 
\end{remark}

\begin{remark}
  The variable $t$ used in~\eqref{e:3Idef} is a ``dummy'' integration variable.
  Namely one can write
  \begin{equation*}
    \int_0^T \Delta(t) \, dW(t)
      = \int_0^T \Delta(s) \, dW(s)
      = \int_0^T \Delta(r) \, dW(r)
      \,,
  \end{equation*}
  or any other variable of your choice.
\end{remark}

\begin{corollary}[It\^o Isometry]
  If~\eqref{e:3Dsq} holds then
  \begin{equation*}
    \E \paren[\Big]{ \int_0^T \Delta(t) \, dW(t) }^2
      = \E \int_0^T \Delta(t)^2 \, dt\,.
  \end{equation*}
\end{corollary}

\begin{proposition}[Linearity]
  If $\Delta_1$ and $\Delta_2$ are two adapted processes, and $\alpha \in \R$, then
  \begin{equation*}
    \int_0^T (\Delta_1(t) + \alpha \Delta_2(t)) \, dW(t) 
      \leq \int_0^T \Delta_1(t) \, dW(t) + \alpha \int_0^T \Delta_2(t) \, dW(t)\,.
  \end{equation*}
\end{proposition}
\begin{remark}
  Positivity, however, is not preserved by It\^o integrals.
  Namely if $\Delta_1 \leq \Delta_2$, there is no reason to expect $\int_0^T \Delta_1(t) \, dW(t) \leq \int_0^T \Delta_2(t) \, dW(t)$.
  Indeed choosing $\Delta_1 = 0$ and $\Delta_2 = 1$ we see that we can not possibly have $0 = \int_0^T \Delta_1(t) \, dW(t)$ to be almost surely smaller than $W(T) = \int_0^T \Delta_2(t) \, dW(t)$.
\end{remark}


Recall, our starting point in these notes was modelling stock prices as \emph{geometric Brownian motions}, given by the equation
\begin{equation*}
  dS(t) = \alpha S(t) \, dt + \sigma S(t) \, dW(t)\,.
\end{equation*}
After constructing It\^o integrals, we are now in a position to describe what this means.
The above is simply shorthand for saying $S$ is a process that satisfies
\begin{equation*}
  S(T) - S(0) = \int_0^T \alpha S(t) \, dt + \int_0^T \sigma S(t) \, dW(t)\,.
\end{equation*}
The first integral on the right is a standard Riemann integral.
The second integral, representing the noisy fluctuations, is the It\^o integral we just constructed.

Note that the above is a little more complicated than the It\^o integrals we will study first, since the process $S$ (that we're trying to define) also appears as an integrand on the right hand side.
In general, such equations are called \emph{Stochastic differential equations}, and are extremely useful in many contexts.

\section{The It\^o formula}

Using the abstract ``limit'' definition of the It\^o integral, it is hard to compute examples.
For instance, what is
\begin{equation*}
  \int_0^T W(s) \, dW(s) \,?
\end{equation*}
This, as we will shortly, can be computed easily using the It\^o formula (also called the It\^o-Doeblin formula).

Suppose $b$ and $\sigma$ are adapted processes.
(In particular, they could but need not, be random).
Consider a process $X$ defined by
\begin{equation}\label{e:3X}
  X(T) = X(0) + \int_0^T b(t) \, dt + \int_0^T \sigma(t) \, dW(t)\,.
\end{equation}
Note the first integral $\int_0^T b(t) \, dt$ is a regular Riemann integral that can be done directly.
The second integral the It\^o integral we constructed in the previous section.

\begin{definition}
  The process $X$ is called an It\^o process if $X(0)$ is deterministic (not random) and for all $T \geq 0$,
  \begin{equation*}
    \E \int_0^T \sigma(t)^2 \, dt < \infty
    \qquad\text{and}\qquad
    \int_0^T b(t) \, dt < \infty\,.
  \end{equation*}
\end{definition}

\begin{remark}
  The shorthand notation for~\eqref{e:3X} is to write
  \begin{gather}\label{e:3Xd}
    \tag{\ref{e:3X}$'$}
    dX(t) = b(t) \, dt + \sigma(t) \, dW(t)\,.
  \end{gather}
\end{remark}

\begin{proposition}
  The quadratic variation of $X$ is
  \begin{equation}\label{e:3qvX}
    \qv{X}(T) = \int_0^T \sigma(t)^2 \, dt
    \quad\text{almost surely.}
  \end{equation}
\end{proposition}
\begin{proof}
  Define $B$ and $M$ by
  \begin{equation*}
    B(T) = \int_0^T b(t) \, dt
    \qquad\text{and}\qquad
    M(T) = \int_0^T \sigma(t) \, dW(t)\,
  \end{equation*}
  and observe
  \begin{multline}\label{e:3incX}
    (X(s + \delta t) - X(s))^2
      = (B(s+\delta t) - B(s))^2 
	+ (M( s + \delta t) - M(s))^2
	\\
	+ 2 \paren{ B(s + \delta t) - B(s) }
	    \paren{ M(s + \delta t) - M(s) }\,.
  \end{multline}

  The key point is that the increment in $B$ is of order $\delta t$, and the increments in $M$ are of order $\sqrt{\delta t}$.
  Indeed, note
  \begin{equation*}
    \abs{B(s + \delta t) - B(s)} = \abs[\Big]{ \int_s^{s + \delta t} b(t) \, dt}
      \leq (\delta t) \max \abs{b}\,,
  \end{equation*}
  showing that increments of $B$ are of order $\delta t$ as desired.
  We already know $M$ has finite quadratic variation, so already expect the increments of $M$ to be of order $\delta t$.

  Now to compute the quadratic variation of $X$, let $n = \floor{T / \delta t}$, $t_i = i \delta t$ and observe
  \begin{multline*}
    \sum_{i=0}^{n-1} \paren{ X(t_{i+1}) - X( t_i ) }^2
      = \sum_{i=0}^{n-1} \paren{ M(t_{i+1}) - M( t_i ) }^2
    \\
    \mathbin+ \sum_{i=0}^{n-1} \paren{ B(t_{i+1}) - B( t_i ) }^2
	+ 2\sum_{i=0}^{n-1}
	    \paren{ B(t_{i+1}) - B( t_i ) }
	    \paren{ M(t_{i+1}) - M( t_i ) }\,.
  \end{multline*}
  The first sum on the right converges (as $\delta t \to 0$) to $\qv{M}(T)$, which we know is exactly $\int_0^T \sigma(t)^2 \, dt$.
  Each term in the second sum is of order $(\delta t)^2$.
  Since there are $T / \delta t$ terms, the second sum converges to $0$.
  Similarly, each term in the third sum is of order $(\delta t)^{3/2}$ and so converges to $0$ as $\delta t \to 0$.
\end{proof}

\begin{remark}
  It's common to decompose $X = B + M$ where $M$ is a martingale and $B$ has finite first variation.
  Processes that can be decomposed in this form are called \emph{semi-martingales}, and the decomposition is unique.
  The process $M$ is called the martingale part of $X$, and $B$ is called the \emph{bounded variation} part of $X$.
\end{remark}

Given an adapted process $\Delta$, interpret $X$ as the price of an asset, and $\Delta$ as our position on it.
(We could either be long, or short on the asset so $\Delta$ could be positive or negative.)

\begin{definition}
  We define the \remove{It\^o} integral of $\Delta$ with respect to $X$ by
  \begin{equation*}
    \int_0^T \Delta(t) \, dX(t)
      \defeq \int_0^T \Delta(t) b(t) \, dt 
	+ \int_0^T \Delta(t) \sigma(t) \, dW(t)\,.
  \end{equation*}
\end{definition}

As before, $\int_0^T \Delta \, dX$ represents the winnings or profit obtained using the trading strategy $\Delta$.

\begin{remark}
  Note that the first integral on the right $\int_0^T \Delta(t) b(t) \, dt$ is a regular Riemann integral, and the second one is an It\^o integral.
  Recall that It\^o integrals with respect to Brownian motion (i.e.\ integrals of the form $\int_0^t \Delta(s) \, dW(s)$ are martingales).
  Integrals with respect to a general process $X$ are only guaranteed to be martingales if $X$ itself is a martingale (i.e.\ $b = 0$), or if the integrand is $0$.
\end{remark}

\begin{remark}
  If we define $I_P$ by
  \begin{equation*}
    I_P(T)
      = \sum_{i=0}^{n-1} \Delta(t_i) (X(t_{i+1}) - X(t_i))
	+ \Delta(t_n) (X(T) - X(t_n))
    \quad\text{if } T \in [t_n, t_{n+1})\,,
  \end{equation*}
  then $I_P$ converges to the integral $\int_0^T \Delta(t) X(t) \, dt$ defined above.
  This works in the same way as~Theorem~\ref{t:3ipconv}.
\end{remark}


Suppose now $f(t, x)$ is some function.
If $X$ is differentiable as a function of $t$ \emph{(which it most certainly is not)}, then the chain rule gives
\begin{align*}
  f(T, X(T)) - f(0, X(0))
    &= \int_0^T \partial_t \paren[\Big]{ f(t, X(t)) } \, dt
  \\
    &= \int_0^T \partial_t f( t, X(t)) \, dt
      + \int_0^T \partial_x f( t, X(t) ) \, \partial_t X(t) \, dt
  \\
    &= \int_0^T \partial_t f( t, X(t) ) \, dt
      + \int_0^T \partial_x f(t, X(t) ) \, dX(t)\,.
\end{align*}
It\^o process are \emph{almost never} differentiable as a function of time, and so the above has no chance of working.
It turns out, however, that for It\^o process you can make the above work by adding an \emph{It\^o correction} term.
This is the celebrated It\^o formula (more correctly the It\^o-Doeblin\footnote{%
  W. Doeblin was a French-German mathematician who was drafted for military service during the second world war.
  During the war he wrote down his mathematical work and sent it in a sealed envelope to the French Academy of Sciences, because he did not want it to ``fall into the wrong hands''.
  When he was about to be captured by the Germans he burnt his mathematical notes and killed himself.

  The sealed envelope was opened in 2000 which revealed that he had a treatment of stochastic Calculus that was essentially equivalent to It\^o's.
  In posthumous recognition, It\^o's formula is now referred to as the It\^o-Doeblin formula by many authors.
}
formula).

\begin{theorem}[It\^o formula, aka It\^o-Doeblin formula]
  If $f = f(t, x)$ is $C^{1,2}$ function\footnote{%
    Recall a function $f = f(t, x)$ is said to be $C^{1,2}$ if it is $C^1$ in $t$ (i.e.\ differentiable with respect to $t$ and $\partial_t f$ is continuous), and $C^2$ in $x$ (i.e.\ twice differentiable with respect to $x$ and $\partial_x f$, $\partial_x^2 f$ are both continuous).
  }
  then
  \begin{multline}\label{e:3ito}
    f(T, X(T)) - f(0, X(0))
      = \int_0^T \partial_t f( t, X(t) ) \, dt
	+ \int_0^T \partial_x f(t, X(t) ) \, dX(t)
    \\
	+ \frac{1}{2} \int_0^T \partial_x^2 f(t, X(t) \, d\qv{X}(t)\,.
  \end{multline}
\end{theorem}

\begin{remark}
  To clarify notation, $\partial_t f( t, X(t))$ means the following: differentiate~$f(t, x)$ with respect to $t$ (treating $x$ as a constant), and then substitute $x = X(t)$.
  Similarly $\partial_x f( t, X(t))$ means differentiate $f(t, x)$ with respect to $x$, and then substitute $x = X(t)$.
  Finally $\partial_x^2 f(t, X(t))$ means take the second derivative of the function $f(t, x)$ with respect to $x$, and the substitute $x = X(t)$.
\end{remark}
\begin{remark}
  In short hand differential form, this is written as
  \begin{multline}\label{e:3itoD}
    \tag{\ref{e:3ito}$'$}
    df(t, X(t)) = \partial_t f(t, X(t)) \, dt + \partial_x f(t, X(t)) \, dX(t)
    \\
      + \frac{1}{2} \partial_x^2 f(t, X(t)) \, d\qv{X}(t)\,.
  \end{multline}
  The term $\frac{1}{2} \partial_x^2 f \, d\qv{X}(t)$ is an ``extra'' term, and is often referred to as the \emph{It\^o correction term}.
  The It\^o formula is simply a version of the \emph{chain rule} for stochastic processes.
  %For brevity many write
  %\begin{equation*}
  %  df(t, X(t)) = \partial_t f \, dt + \partial_x f \, dX(t) + \frac{1}{2} \partial_x^2 f \, d\qv{X}(t)\,.
  %\end{equation*}
\end{remark}

\begin{remark}
  Substituting what we know about $X$ from~\eqref{e:3X} and~\eqref{e:3qvX} we see that~\eqref{e:3ito} becomes
  \begin{multline*}
    f(T, X(T)) - f(0, X(0))
      = \int_0^T \paren[\big]{ \partial_t f( t, X(t) )
	+ \partial_x f(t, X(t) ) b(t)} \, dt
    \\
      \mathbin+ \int_0^T \partial_x f(t, X(t)) \sigma(t) \, dW(t)
	+ \frac{1}{2} \int_0^T \partial_x^2 f(t, X(t)) \, \sigma(t)^2 \, dt\,.
  \end{multline*}
  The second integral on the right is an It\^o integral (and hence a martingale).
  The other integrals are regular Riemann integrals which yield processes of finite variation.
\end{remark}

While a complete rigorous proof of the It\^o formula is technical, and beyond the scope of this course, we provide a quick heuristic argument that illustrates the main idea clearly.

\begin{proof}[Intuition behind the It\^o formula]
  Suppose that the function $f$ is only a function of $x$ and doesn't depend on $t$, and $X$ is a standard Brownian motion (i.e.\ $b = 0$ and $\sigma = 1$).
  In this case proving It\^o's formula reduces to proving
  \begin{equation*}
    f(W(T)) - f(W(0)) = \int_0^T f'(W(t)) \, dW(t) + \frac{1}{2} \int_0^T f''(W(t)) \, dt \,.
  \end{equation*}

  Let $P = \set{0 = t_0 < t_1 < \cdots < t_n = T}$ be a partition of $[0, T]$.
  Taylor expanding $f$ to second order gives
  \begin{multline}\label{e:3itoProof1}
    f(W(T)) - f(W(0))
      = \sum_{i=0}^{n-1} f(W(t_{i+1})) - f(W(t_i))
    \\
      = \sum_{i=0}^{n-1} f'(W(t_i)) (W(t_{i+1}) - W(t_i))
	+ \frac{1}{2} \sum_{i=0}^{n-1} f''(W(t_i)) (W( t_{i+1}) - W(t_i))^2
    \\
	+ \frac{1}{2} \sum_{i=0}^{n-1} o\paren[\big]{ (W( t_{i+1}) - W(t_i))^2 } \,,
  \end{multline}
  where the last sum on the right is the remainder from the Taylor expansion.

  Note the first sum on the right of~\eqref{e:3itoProof1} converges to the It\^o integral
  \begin{equation*}
    \int_0^T f'(W(t)) \, dW(t)\,.
  \end{equation*}
  For the second sum on the right of~\eqref{e:3itoProof1}, note
  \begin{multline*}
    f''(W(t_i)) (W( t_{i+1}) - W(t_i))^2
      = f''(W(t_i)) (t_{i+1} - t_i)
    \\
	+ f''(W(t_i))\brak[\big]{(W( t_{i+1}) - W(t_i))^2 - (t_{i+1} - t_i)}
  \end{multline*}
  After summing over $i$, first term on the right converges to the Riemann integral $\int_0^T f''(W(t)) \, dt$.
  The second term on the right is similar to what we had when computing the quadratic variation of $W$.
  The variance of $(W( t_{i+1}) - W(t_i))^2 - (t_{i+1} - t_i)$ is of order $(t_{i+1} - t_i)^2$.
  Thus we expect that the second term above, when summed over $i$, converges to $0$.

  Finally each summand in the remainder term (the last term on the right of~\eqref{e:3itoProof1}) is smaller than $(W(t_{i+1}) - W(t_i))^2$.
  (If, for instance, $f$ is three times continuously differentiable in $x$, then each summand in the remainder term is of order $(W(t_{i+1}) - W(t_i))^3$.)
  Consequently, when summed over~$i$ this should converge to $0$ 
\end{proof}

\section{A few examples using It\^o's formula}

Technically, as soon as you know It\^o's formula you can ``jump right in'' and derive the Black-Scholes equation.
However, because of the importance of It\^o's formula, we work out a few simpler examples first.

\begin{example}
  Compute the quadratic variation of $W(t)^2$.
\end{example}
\begin{sol}
  Let $f(t, x) = x^2$.
  Then, by It\^o's formula,
  \begin{align*}
    d \paren[\big]{ W(t)^2 }
      &= d f(t, W(t))
    \\
      &= \partial_t f( t, W(t)) \, dt + \partial_x f (t, W(t)) \, dW(t)
	+ \frac{1}{2} \partial_x^2 f(t, W(t)) \, dt
    \\
      &= 2 W(t) \, dW(t) + dt\,.
  \end{align*}
  Or, in integral form,
  \begin{equation*}
    W(T)^2 - W(0)^2
      = W(T)^2
      = 2 \int_0^T W(t) \, dW(t) + T\,.
  \end{equation*}
  Now the second term on the right has finite first variation, and won't affect our computations for quadratic variation.
  The first term is an martingale whose quadratic variation is $\int_0^T W(t)^2 \, dt$, and so
  \begin{equation*}
    \qv{W^2}(T) = 4 \int_0^T W(t)^2 \, dt \,.
    \qedhere
  \end{equation*}
\end{sol}
\begin{remark}
  Note the above also tells you
  \begin{equation*}
    2 \int_0^T W(t) \, dW(t) = W(T)^2 - T \,.
  \end{equation*}
\end{remark}

\begin{example}
  Let $M(t) = W(t)$ and $N(t) = W(t)^2 - t$.
  We know $M$ and $N$ are martingales.
  Is $M N$ a martingale?
\end{example}
\begin{sol}
  Note $M(t) N(t) = W(t)^3 - t W(t)$.
  By It\^o's formula,
  \begin{equation*}
    d(MN) = - W(t) \, dt + (3 W(t)^2 - t) \, dW(t) + 3 W(t) \, dt \,.
  \end{equation*}
  Or in integral form
  \begin{equation*}
    M(t) N(t) = \int_0^t 2 W(s) \, ds + \int_0^t (3 W(s)^2 - s) \, dW(s)\,.
  \end{equation*}
  Now the second integral on the right is a martingale, but the first integral most certainly is not.
  So $MN$ can not be a martingale.
\end{sol}
\begin{remark}
  Note, above we changed the integration variable from $t$ to $s$.
  This is perfectly legal -- the variable with which you integrate with respect to is a dummy variable (just line regular Riemann integrals) and you can replace it what your favourite (unused!) symbol.
\end{remark}
\begin{remark}
  It's worth pointing out that the It\^o integral $\int_0^t \Delta(s) \, dW(s)$ is always a martingale (under the finiteness condition~\eqref{e:3Dsq}).
  However, the Riemann integral $\int_0^t b(s) \, ds$ is only a martingale if $b = 0$ identically.
\end{remark}

\begin{proposition}
  If $f = f(t, x)$ is $C^{1,2}_b$ then the process
  \begin{equation*}
    M(t) \defeq f(t, W(t))
      - \int_0^t \paren[\Big]{
	  \partial_t f(s, W(s)) + \frac{1}{2} \partial_x^2 f(s, W(s))
	} \, ds
  \end{equation*}
  is a martingale.
\end{proposition}
\begin{remark}
  We'd seen this earlier, and the proof involved computing the conditional expectations directly and checking an algebraic identity involving the density of the normal distribution.
  With It\^o's formula, the proof is ``immediate''.
\end{remark}
\begin{proof}
  By It\^o's formula (in integral form)
  \begin{align*}
    \MoveEqLeft
    f(t, W(t)) - f(0, W(0))
    \\
      &= \int_0^t \partial_t f(s, W(s)) \, ds
	+ \int_0^t \partial_x f( s, W(s)) \, dW(s)
	+ \frac{1}{2} \int_0^t \partial_x^2 f(s, W(s)) \, ds
    \\
      &= \int_0^t \paren[\Big]{
	      \partial_t f(s, W(s))
	      + \frac{1}{2} \partial_x^2 f(s, W(s))
	    } \, ds
	  + \int_0^t \partial_x f( s, W(s)) \, dW(s)\,.
  \end{align*}
  Substituting this we see
  \begin{equation*}
    M(t) = f(0, W(0))
	  + \int_0^t \partial_x f( s, W(s)) \, dW(s)\,,
  \end{equation*}
  which is a martingale.
\end{proof}
\begin{remark}
  Note we said $f \in C^{1,2}_b$ to ``cover our bases''.
  Recall for It\^o integrals to be martingales, we need the finiteness condition~\eqref{e:3Dsq} to hold.
  This will certainly be the case if $\partial_x f$ is bounded, which is why we made this assumption.
  The result above is of course true under much more general assumptions.
\end{remark}

\begin{example}
  Let $X(t) = t \sin( W(t) )$. Is $X^2 - \qv{X}$ a martingale?
\end{example}
\begin{sol}
  We compute
  \begin{equation*}
    dX(t) = \sin(W(t)) \, dt + t \cos( W(t)) \, dW(t) - \frac{1}{2} t \sin^2(W(t)) \, dt\,,
  \end{equation*}
  and so
  \begin{equation*}
    d\qv{X}(t) = t^2 \cos^2(W(t)) \, dt\,.
  \end{equation*}
  Also
  \begin{equation*}
    d X(t)^2
      = 2 X(t) \, dX(t) + d\qv{X}(t)
  \end{equation*}
  and so
  \begin{multline*}
    d( X(t)^2 - \qv{X} )
      = 2 X(t) \, dX(t)
    \\
      = 2t \sin(t) \paren[\big]{ \sin(W(t)) - \frac{t \sin(W(t))^2 }{2} } \, dt
	+ 2t \sin(t) \paren[\big]{ t \cos(W(t)) } \, dW(t)\,.
  \end{multline*}
  Since the $dt$ term above isn't $0$, $X(t)^2 - \qv{X}$ can not be a martingale.
\end{sol}

Recall we said earlier (Theorem~\ref{t:3qv1}) that for any martingale $M$, $M^2 - \qv{M}$ is a martingale.
We used this fact in the construction of It\^o integrals, and the proof of It\^o's formula.
However, once we have It\^o's formula, this fact can be deduced quickly for It\^o processes.

\begin{proposition}
  Let $M(t)= \int_0^t \sigma(s) \, dW(s)$.
  Then $M^2 - \qv{M}$ is a martingale.
\end{proposition}
\begin{proof}
  Let $N(t) = X(t)^2 - \qv{X}(t)$.
  Observe that by It\^o's formula,
  \begin{equation*}
    d( X(t)^2 ) = 2 X(t) \, dX(t) + d\qv{X}(t)\,.
  \end{equation*}
  Hence
  \begin{equation*}
    d N = 2 X(t) dX(t) + d\qv{X}(t) - d\qv{X}(t)
      = 2X(t) \sigma(t) \, dW(t)\,,
  \end{equation*}
  which is a martingale.
\end{proof}

\section{Review Problems}

\begin{problem}
  If $0 \leq r < s < t$, compute $\E \paren[\big]{ W(r) W(s) W(t)}$.
\end{problem}

\begin{problem}
  Define the processes $X, Y, Z$ by
  \begin{equation*}
    X(t) = \int_0^{W(t)} e^{-W(s)^2} \, ds\,,
    \quad
    Y(t) = \exp\paren[\Big]{ \int_0^t W(s) \, ds }\,,
    \quad
    Z(t) = \frac{t}{X(t)}\,.
  \end{equation*}
  Decompose each of these processes as the sum of a martingale and a process of finite first variation.
  What is the quadratic variation of each of these processes?
\end{problem}

\begin{problem}
  Define the processes $X, Y$ by
  \begin{equation*}
    X(t) \defeq \int_0^t W(s) \, ds\,, \quad
    Y(t) \defeq \int_0^t W(s) \, dW(s)\,.
  \end{equation*}
  Given $0 \leq s < t$, compute the conditional expectations
  $\E( X(t) \given \mathcal F_s )$, 
  and
  $\E( Y(t) \given \mathcal F_s )$.
\end{problem}

\begin{problem}
  Let $\displaystyle M(t) = \int_0^t W(s) \, dW(s)$.
  Find a function $f$ such that
  \begin{equation*}
    E(t) \defeq \exp\paren[\Big]{ M(t) - \int_0^t f(s, W(s)) \, ds }
  \end{equation*}
  is a martingale.
\end{problem}
\begin{problem}
  Suppose $\sigma = \sigma(t)$ is a deterministic (i.e.\ non-random) process, and $X$ is the It\^o process defined by
  \begin{equation*}
    X(t) = \int_0^t \sigma(u) \, dW(u)\,.
  \end{equation*}
  \vspace{-.7\baselineskip}
  \begin{parts}
    \item
      Given $\lambda, s, t \in \R$ with $0 \leq s < t$ compute $\E \paren{ e^{\lambda (X(t) - X(s))} \given \mathcal F_s}$.

    \item
      If $r \leq s$ compute $\E \exp\paren{\lambda X(r) + \mu(X(t) - X(s))}$.

    \item
      What is the joint distribution of $(X(r), X(t) - X(s))$?

    \item
      \emph{(L\'evy's criterion)}
      If $\sigma(u) = \pm 1$ for all $u$, then show that $X$ is a standard Brownian motion.
  \end{parts}
\end{problem}

\begin{problem}
  Let $\displaystyle M(t) = \int_0^t W(s) \, dW(s)$.
  For $s < t$, is $M(t) - M(s)$ independent of $\mathcal F_s$?
  Justify.
\end{problem}

\begin{problem}
  Determine whether the following identities are true or false, and justify your answer.
  \begin{parts}
    \item
      $\displaystyle e^{2t} \sin(2 W(t)) = 2 \int_0^t e^{2s} \cos(2 W(s)) \, dW(s)$.

    \item
      $\displaystyle \abs{W(t)} = \int_0^t \sign(W(s)) \, dW(s)$.
      (Recall $\sign(x) = 1$ if $x > 0$, $\sign(x) = -1$ if $x < 0$ and $\sign(x) = 0$ if $x = 0$.)
  \end{parts}
\end{problem}
\end{document}
