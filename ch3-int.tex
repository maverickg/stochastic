%&subfile
\begin{document}
\ifstandalonechapter\setcounter{chapter}{2}\fi
\chapter{Stochastic Integration}

\section{Motivation}
Suppose $\Delta(t)$ is your position at time $t$ on a security whose price is $S(t)$.
If you only trade this security at times $0 = t_0 < t_1 < t_2 < \cdots < t_n = T$, then the value of your winnings up to time $T$ is exactly
\begin{equation*}
  X(t_n) = \sum_{i=0}^{n-1} \Delta(t_i) ( S(t_{i+1}) - S(t_i) )
\end{equation*}
If you are trading this continuously in time, you'd expect that a ``simple'' limiting procedure should show that your winnings are given by the \emph{Riemann-Stieltjes} integral:
\begin{equation*}%\label{e:3stLim}
  X(T) = 
    \lim_{\norm{P} \to 0}
      \sum_{i=0}^{n-1} \Delta(t_i) \paren{ S( t_{i+1} ) - S( t_i ) }
    = \int_0^T \Delta(t) \, dS(t) \,.
\end{equation*}
Here $P = \set{0 = t_0 < \cdots < t_n = T}$ is a partition of $[0, T]$, and $\norm{P} = \max\set{ t_{i+1} - t_i }$.

%The reason we have $X(\xi_i)$ above, and not $X(t_i)$ is because the standard definition of the \emph{Riemann-Stieltjes} allows you to choose \emph{arbitrary} times in the partition sub-intervals $[t_i, t_{i+1}]$.
%In our context, this doesn't sit right: If you trade at time $t_i$, you should do so using only information available to you up to time $t_i$ (i.e.\ be $\mathcal F_{t_i}$-measurable), and your winnings are $X(t_i) (S(t_{i+1}) - S(t_i) )$ and not $X(\xi_i) (S(t_{i+1}) - S(t_i) )$.
%Indeed, it turns out that if $S$ is a continuous martingale, then the limit of the form~\eqref{e:3stLim} \emph{will not} exist in general.

This has been well studied by mathematicians, and it is well known that for the above limiting procedure to ``work directly'', you need $S$ to have finite \emph{first variation}.
Recall, the \emph{first variation} of a function is defined to be
\begin{equation*}
  V_{[0,T]}(S) \defeq \lim_{\norm{P} \to 0} \sum_{i=0}^{n-1} \abs{S(t_{i+1}) - S(t_i)}\,.
\end{equation*}
It turns out that almost \emph{any} continuous martingale~$S$ \emph{will not} have finite first variation.
Thus to define integrals with respect to martingales, one has to do something `clever'.
It turns out that if $X$ is adapted and $S$ is an martingale, then the above limiting procedure works, and this was carried out by It\^o (and independently by Doeblin).

\section{The First Variation of Brownian motion}

We begin by showing that the first variation of Brownian motion is infinite.
\begin{proposition}
  If $W$ is a standard Brownian motion, and $T > 0$ then
  \begin{equation*}
    \lim_{n \to \infty} \E \sum_{k=0}^{n-1}
      \abs[\Big]{
	W\paren[\Big]{\frac{k+1}{n}}
	- W\paren[\Big]{\frac{k}{n}}
      } = \infty\,.
  \end{equation*}
\end{proposition}
\begin{remark}
  In fact
  \begin{equation*}
    \lim_{n \to \infty} \sum_{k=0}^{n-1}
      \abs[\Big]{
	W\paren[\Big]{\frac{k+1}{n}}
	- W\paren[\Big]{\frac{k}{n}}
      } = \infty
      \quad\text{almost surely,}
  \end{equation*}
  but this won't be necessary for our purposes.
\end{remark}
\begin{proof}
  Since $W( (k+1)/n ) - W(k/n) \sim N(0, 1/n)$ we know
  \begin{equation*}
    \E \abs[\Big]{
	W\paren[\Big]{\frac{k+1}{n}}
	- W\paren[\Big]{\frac{k}{n}}
      }
      = \int_\R \abs{x} \, G\paren[\Big]{ \frac{1}{n}, x } \, dx
      = \frac{C}{\sqrt{n}}\,,
  \end{equation*}
  where
  \begin{equation*}
     C = \int_\R \abs{y} e^{-y^2/2} \, \frac{dy}{\sqrt{2\pi}}
      = \E \abs{N(0, 1)} \,.
  \end{equation*}
  Consequently
  \begin{equation*}
    \sum_{k=0}^{n-1}
      \E
      \abs[\Big]{
	W\paren[\Big]{\frac{k+1}{n}}
	- W\paren[\Big]{\frac{k}{n}}
      }
      = \frac{C n}{\sqrt{n}}
      \xrightarrow{n \to \infty} \infty\,.
      \qedhere
  \end{equation*}
\end{proof}

\section{Quadratic Variation}

It turns out that the \emph{second} variation of any \emph{square integrable} martingale is almost surely finite, and this is the key step in constructing the It\^o integral.
\begin{definition}
  Let $M$ be any process.
  We define the \emph{quadratic variation} of $M$, denoted by $\qv{M}$ by
  \begin{equation*}
    \qv{M}(T) = \lim_{\norm{P} \to 0} (M(t_{i+1}) - M(t_i))^2\,,
  \end{equation*}
  where $P = \set{0 = t_1 < t_1 \cdots < t_n = T}$ is a partition of $[0, T]$.
\end{definition}

\begin{proposition}
  If $W$ is a standard Brownian motion, then $\qv{W}(T) = T$ almost surely.
\end{proposition}
\begin{proof}
  For simplicity, let's assume $t_i = Ti/n$.
  Note
  \begin{equation*}
    \sum_{i=0}^{n-1}
      \paren[\Big]{
	W\paren[\Big]{\frac{(i+1)T}{n}}
	- W\paren[\Big]{\frac{iT}{n}}
      }^2 - T
      = \sum_{i=0}^{n-1}
	  \xi_i\,,
  \end{equation*}
  where
  \begin{equation*}
    \xi_i \defeq
	    \paren[\Big]{
	      W\paren[\Big]{\frac{(i+1)T}{n}}
	      - W\paren[\Big]{\frac{iT}{n}}
	    }^2 - \frac{T}{n}\,.
  \end{equation*}
  Note that $\xi_i$'s are i.i.d.\ with distribution $N(0, T/n)^2 - T/n$, and hence
  \begin{equation*}
    \E \xi_i = 0
    \qquad\text{and}\qquad
    \var \xi_i = \frac{T^2 (\E N(0,1)^4 - 1)}{n^2}.
  \end{equation*}
  Consequently
  \begin{equation*}
    \var\paren[\Big]{
      \sum_{i=0}^{n-1} \xi_i
    } = \frac{T^2 (\E N(0,1)^4 - 1)}{n} \xrightarrow{n \to \infty} 0\,,
  \end{equation*}
  which shows
  \begin{equation*}
    \sum_{i=0}^{n-1}
      \paren[\Big]{
	W\paren[\Big]{\frac{(i+1)T}{n}}
	- W\paren[\Big]{\frac{iT}{n}}
      }^2 - T
      = \sum_{i=0}^{n-1}
	  \xi_i
      \xrightarrow{n \to \infty} 0 \,.
      \qedhere
  \end{equation*}
\end{proof}

\begin{corollary}
  The process $M(t) \defeq W(t)^2 - \qv{W}(t)$ is a martingale.
\end{corollary}
\begin{proof}
  We see
  \begin{multline*}
    \E( W(t)^2 - t \given \mathcal F_s )
      = \E ( (W(t) - W(s))^2  + 2 W(s) (W(t) - W(s)) + W(s)^2 \given \mathcal F_s ) - t
      \\
      = W(s)^2 - s\,
  \end{multline*}
  and hence $\E (M(t) \given \mathcal F_s ) = M(s)$.
\end{proof}

The above wasn't a co-incidence.
This property in fact characterizes the quadratic variation.
\begin{theorem}\label{t:3qv1}
  Let $M$ be a continuous martingale with respect to a filtration~$\set{\mathcal F_t}$.
  Then $\E M(t)^2 < \infty$ if and only if $\E \qv{M}(t) < \infty$.
  In this case the process $M(t)^2 - \qv{M}(t)$ is also a martingale with respect to the same filtration, and hence $\E M(t)^2 - \E M(0)^2 = \E \qv{M}(t)$.
\end{theorem}

The above is in fact a characterization of the quadratic variation of martingales.

\begin{theorem}\label{t:3qv2}
  If $A(t)$ is any continuous, increasing, adapted process such that $M(t)^2 - A(t)$ is a martingale, then $A = \qv{M}$.
\end{theorem}

The proof of these theorems are a bit technical and go beyond the scope of these notes.
The results themselves, however, are extremely important and will be used subsequently.

\begin{remark}
  The intuition to keep in mind about the first variation and the quadratic variation is the following.
  Divide the interval $[0, T]$ into $T/\delta t$ intervals of size $\delta t$.
  If $X$ has finite first variation, then on each subinterval $(k \delta t, (k+1) \delta t)$ the increment of $X$ should be of order $\delta t$.
  Thus adding $T / \delta t$ terms of order $\delta t$ will yield something finite.

  On the other hand if $X$ has finite quadratic variation, on each subinterval $(k \delta t, (k+1) \delta t)$ the increment of $X$ should be of order $\sqrt{\delta t}$, so that adding $T / \delta t$ terms of the \emph{square} of the increment yields something finite.
  Doing a quick check for Brownian motion (which has finite quadratic variation), we see
  \begin{equation*}
    \E \abs{W(t + \delta t) - W(t)} = \sqrt{\delta t} \E \abs{N(0, 1)}\,,
    %\quad\text{and}\quad
    %\E \abs{W(t + \delta t) - W(t)}^2 = \delta t\,,
  \end{equation*}
  which is in line with our intuition.
\end{remark}

\begin{remark}
  If a continuous process has finite first variation, its quadratic variation will necessarily be $0$.
  On the other hand, if a continuous process has finite (and non-zero) quadratic variation, its first variation will necessary be infinite.
\end{remark}


\section{Construction of the It\^o integral}

Let $W$ be a standard Brownian motion, $\set{\mathcal F_t}$ be the Brownian filtration and $\Delta$ be an adapted process.
We think of $\Delta(t)$ to represent our position at time $t$ on an asset whose spot price is $W(t)$.

\begin{lemma}\label{l:3dito}
Let $P = \set{0 = t_0 < t_1 < t_2 < \cdots}$ be an increasing sequence of times, and assume $\Delta$ is constant on $[t_i, t_{i+1})$ (i.e.\ the asset is only traded at times $t_0$, \dots, $t_n$).
Let $I_P(T)$, defined by
\begin{equation*}
  I_P(T)
    = \sum_{i=0}^{n-1} \Delta(t_i) (W(t_{i+1}) - W(t_i))
      + \Delta(t_n) (W(T) - W(t_n))
  \quad\text{if } T \in [t_n, t_{n+1})\,.
\end{equation*}
be your cumulative winnings up to time $T$.
Then,
\begin{equation}\label{e:3Eip2}
  \E I_P(T)^2
    = \E \brak[\Big]{ \sum_{i=0}^{n} \Delta(t_i)^2 (t_{i+1} - t_i)
      + \Delta(t_n)^2 (T - t_n) }
  \quad\text{if } T \in [t_n, t_{n+1})\,.
\end{equation}
Moreover, $I_P$ is a martingale and
\begin{equation}\label{e:3dqv}
  \qv{I_P}(T) = \sum_{i=0}^{n} \Delta(t_i)^2 (t_{i+1} - t_i)
      + \Delta(t_n)^2 (T - t_n)
  \quad\text{if } T \in [t_n, t_{n+1})\,.
\end{equation}
\end{lemma}

This lemma, as we will shortly see, is the key to the construction of stochastic integrals (called It\^o integrals).
\begin{proof}%[Proof of Lemma~\ref{l:3dito}]
  We first prove~\eqref{e:3Eip2} with $T = t_n$ for simplicity.
  Note
  \begin{multline}\label{e:3sum1}
    \E I_P(t_n)^2 
      = \sum_{i=0}^{n-1}
	  \E \Delta(t_i)^2 (W(t_{i+1}) - W(t_i))^2
    \\
	+ 2 \sum_{j=0}^{n-1}
	  \sum_{i=0}^{i-1}
	  \E \Delta(t_i) \Delta(t_j)
	    (W(t_{i+1}) - W(t_i))
	    (W(t_{j+1}) - W(t_j))
  \end{multline}
  By the tower property
  \begin{multline*}
    \E \Delta(t_i)^2 (W(t_{i+1}) - W(t_i))^2
      = \E \E \paren[\big]{ \Delta(t_i)^2 (W(t_{i+1}) - W(t_i))^2 \given \mathcal F_{t_i} }
    \\
      = \E \Delta(t_i)^2 \E \paren[\big]{ (W(t_{i+1}) - W(t_i))^2 \given \mathcal F_{t_i} }
      = \E \Delta(t_i)^2 (t_{i+1} - t_i)\,.
  \end{multline*}
  Similarly we compute
  \begin{align*}
    \MoveEqLeft
    \E \Delta(t_i) \Delta(t_j)
      (W(t_{i+1}) - W(t_i))
      (W(t_{j+1}) - W(t_j))
    \\
    &= \E \E \paren[\big]{ \Delta(t_i) \Delta(t_j)
      (W(t_{i+1}) - W(t_i))
      (W(t_{j+1}) - W(t_j)) \given \mathcal F_{t_j} }
    \\
    &= \E \Delta(t_i) \Delta(t_j)
	  (W(t_{i+1}) - W(t_i))
	  \E \paren[\big]{ (W(t_{j+1}) - W(t_j)) \given \mathcal F_{t_j} }
    = 0\,.
  \end{align*}
  Substituting these in~\eqref{e:3sum1} immediately yields~\eqref{e:3Eip2} for $t_n = T$.

  The proof that $I_P$ is an martingale uses the same ``tower property'' idea, and is left to the reader to check.
  The proof of~\eqref{e:3dqv} is also similar in spirit, but has a few more details to check.
  The main idea is to let $A(t)$ be the right hand side of~\eqref{e:3dqv}.
  Observe~$A$ is clearly a continuous, increasing, adapted process.
  Thus, if we show $M^2 - A$ is a martingale, then using Theorem~\ref{t:3qv2} we will have $A = \qv{M}$ as desired.
  The proof that $M^2 - A$ is an martingale uses the same ``tower property'' idea, but is a little more technical and is left to the reader.
\end{proof}

Note that as $\norm{P} \to 0$, the right hand side of~\eqref{e:3dqv} converges to the standard Riemann integral $\int_0^T \Delta(t)^2 \, dt$.
It\^o realised he could use this to prove that $I_P$ itself converges, and the limit is now called the It\^o integral.

\begin{theorem}\label{t:3ipconv}
  If $\int_0^T \Delta(t)^2 \, dt < \infty$ almost surely, then as $\norm{P} \to 0$, the processes $I_P$ converge to a \emph{continuous process} $I$ denoted by
  \begin{equation}\label{e:3Idef}
    I(T)
      \defeq \lim_{\norm{P} \to 0} I_P(T)\,
      \defeq \int_0^T \Delta(t) \, dW(t)\,.
  \end{equation}
  This is known as the \emph{It\^o integral of $\Delta$ with respect to $W$}.
  If further
  \begin{equation}\label{e:3Dsq}
    \E \int_0^T \Delta(t)^2 \, dt < \infty\,,
  \end{equation}
  then the process $I(T)$ is a \emph{martingale} and the quadratic variation $\qv{I}$ satisfies
  \begin{equation*}
    \qv{I}(T) = \int_0^T \Delta(t)^2 \, dt
    \quad\text{almost surely.}
  \end{equation*}
\end{theorem}

\begin{remark}
  For the above to work, it is \emph{crucial} that $\Delta$ is adapted, and is sampled at the left endpoint of the time intervals.
  That is, the terms in the sum are $\Delta(t_i) (W(t_{i+1}) - W(t_i))$, and not $\Delta(t_{i+1}) (W(t_{i+1}) - W(t_i))$ or $\frac{1}{2} (\Delta(t_i) + \Delta(t_{i+1})) (W(t_{i+1}) - W(t_i))$, or something else.

  Usually if the process is not adapted, there is no meaningful way to make sense of the limit.
  However, if you sample at different points, it still works out (usually) but what you get is \emph{different} from the It\^o integral (one example is the Stratonovich integral). 
\end{remark}

\begin{remark}
  The variable $t$ used in~\eqref{e:3Idef} is a ``dummy'' integration variable.
  Namely one can write
  \begin{equation*}
    \int_0^T \Delta(t) \, dW(t)
      = \int_0^T \Delta(s) \, dW(s)
      = \int_0^T \Delta(r) \, dW(r)
      \,,
  \end{equation*}
  or any other variable of your choice.
\end{remark}

\begin{corollary}[It\^o Isometry]
  If~\eqref{e:3Dsq} holds then
  \begin{equation*}
    \E \paren[\Big]{ \int_0^T \Delta(t) \, dW(t) }^2
      = \E \int_0^T \Delta(t)^2 \, dt\,.
  \end{equation*}
\end{corollary}

\begin{proposition}[Linearity]
  If $\Delta_1$ and $\Delta_2$ are two adapted processes, and $\alpha \in \R$, then
  \begin{equation*}
    \int_0^T (\Delta_1(t) + \alpha \Delta_2(t)) \, dW(t) 
      \leq \int_0^T \Delta_1(t) \, dW(t) + \alpha \int_0^T \Delta_2(t) \, dW(t)\,.
  \end{equation*}
\end{proposition}
\begin{remark}
  Positivity, however, is not preserved by It\^o integrals.
  Namely if $\Delta_1 \leq \Delta_2$, there is no reason to expect $\int_0^T \Delta_1(t) \, dW(t) \leq \int_0^T \Delta_2(t) \, dW(t)$.
  Indeed choosing $\Delta_1 = 0$ and $\Delta_2 = 1$ we see that we can not possibly have $0 = \int_0^T \Delta_1(t) \, dW(t)$ to be almost surely smaller than $W(T) = \int_0^T \Delta_2(t) \, dW(t)$.
\end{remark}


Recall, our starting point in these notes was modelling stock prices as \emph{geometric Brownian motions}, given by the equation
\begin{equation*}
  dS(t) = \alpha S(t) \, dt + \sigma S(t) \, dW(t)\,.
\end{equation*}
After constructing It\^o integrals, we are now in a position to describe what this means.
The above is simply shorthand for saying $S$ is a process that satisfies
\begin{equation*}
  S(T) - S(0) = \int_0^T \alpha S(t) \, dt + \int_0^T \sigma S(t) \, dW(t)\,.
\end{equation*}
The first integral on the right is a standard Riemann integral.
The second integral, representing the noisy fluctuations, is the It\^o integral we just constructed.

Note that the above is a little more complicated than the It\^o integrals we will study first, since the process $S$ (that we're trying to define) also appears as an integrand on the right hand side.
In general, such equations are called \emph{Stochastic differential equations}, and are extremely useful in many contexts.

\section{The It\^o formula}

Using the abstract ``limit'' definition of the It\^o integral, it is hard to compute examples.
For instance, what is
\begin{equation*}
  \int_0^T W(s) \, dW(s) \,?
\end{equation*}
This, as we will shortly, can be computed easily using the It\^o formula (also called the It\^o-Doeblin formula).

Suppose $b$ and $\sigma$ are adapted processes.
(In particular, they could but need not, be random).
Consider a process $X$ defined by
\begin{equation}\label{e:3X}
  X(T) = X(0) + \int_0^T b(t) \, dt + \int_0^T \sigma(t) \, dW(t)\,.
\end{equation}
Note the first integral $\int_0^T b(t) \, dt$ is a regular Riemann integral that can be done directly.
The second integral the It\^o integral we constructed in the previous section.

\begin{definition}
  The process $X$ is called an It\^o process if $X(0)$ is deterministic (not random) and for all $T \geq 0$,
  \begin{equation*}
    \E \int_0^T \sigma(t)^2 \, dt < \infty
    \qquad\text{and}\qquad
    \int_0^T b(t) \, dt < \infty\,.
  \end{equation*}
\end{definition}

\begin{remark}
  The shorthand notation for~\eqref{e:3X} is to write
  \begin{gather}\label{e:3Xd}
    \tag{\ref{e:3X}$'$}
    dX(t) = b(t) \, dt + \sigma(t) \, dW(t)\,.
  \end{gather}
\end{remark}

\begin{proposition}
  The quadratic variation of $X$ is
  \begin{equation}\label{e:3qvX}
    \qv{X}(T) = \int_0^T \sigma(t)^2 \, dt
    \quad\text{almost surely.}
  \end{equation}
\end{proposition}
\begin{proof}
  Define $B$ and $M$ by
  \begin{equation*}
    B(T) = \int_0^T b(t) \, dt
    \qquad\text{and}\qquad
    M(T) = \int_0^T \sigma(t) \, dW(t)\,
  \end{equation*}
  and observe
  \begin{multline}\label{e:3incX}
    (X(s + \delta t) - X(s))^2
      = (B(s+\delta t) - B(s))^2 
	+ (M( s + \delta t) - M(s))^2
	\\
	+ 2 \paren{ B(s + \delta t) - B(s) }
	    \paren{ M(s + \delta t) - M(s) }\,.
  \end{multline}

  The key point is that the increment in $B$ is of order $\delta t$, and the increments in $M$ are of order $\sqrt{\delta t}$.
  Indeed, note
  \begin{equation*}
    \abs{B(s + \delta t) - B(s)} = \abs[\Big]{ \int_s^{s + \delta t} b(t) \, dt}
      \leq (\delta t) \max \abs{b}\,,
  \end{equation*}
  showing that increments of $B$ are of order $\delta t$ as desired.
  We already know $M$ has finite quadratic variation, so already expect the increments of $M$ to be of order $\delta t$.

  Now to compute the quadratic variation of $X$, let $n = \floor{T / \delta t}$, $t_i = i \delta t$ and observe
  \begin{multline*}
    \sum_{i=0}^{n-1} \paren{ X(t_{i+1}) - X( t_i ) }^2
      = \sum_{i=0}^{n-1} \paren{ M(t_{i+1}) - M( t_i ) }^2
    \\
    \mathbin+ \sum_{i=0}^{n-1} \paren{ B(t_{i+1}) - B( t_i ) }^2
	+ 2\sum_{i=0}^{n-1}
	    \paren{ B(t_{i+1}) - B( t_i ) }
	    \paren{ M(t_{i+1}) - M( t_i ) }\,.
  \end{multline*}
  The first sum on the right converges (as $\delta t \to 0$) to $\qv{M}(T)$, which we know is exactly $\int_0^T \sigma(t)^2 \, dt$.
  Each term in the second sum is of order $(\delta t)^2$.
  Since there are $T / \delta t$ terms, the second sum converges to $0$.
  Similarly, each term in the third sum is of order $(\delta t)^{3/2}$ and so converges to $0$ as $\delta t \to 0$.
\end{proof}

\begin{remark}
  It's common to decompose $X = B + M$ where $M$ is a martingale and $B$ has finite first variation.
  Processes that can be decomposed in this form are called \emph{semi-martingales}, and the decomposition is unique.
  The process $M$ is called the martingale part of $X$, and $B$ is called the \emph{bounded variation} part of $X$.
\end{remark}

\begin{proposition}\label{p:3smuniq}
  The semi-martingale decomposition of $X$ is unique.
  That is, if $X = B_1 + M_1 = B_2 + M_2$ where $B_1, B_2$ are continuous adapted processes with bounded variation, and $M_1, M_2$ are continuous (square integrable) martingales, then $B_1 = B_2$ and $M_1 = M_2$.
\end{proposition}
\begin{proof}
  Set $M = M_1 - M_2$ and $B = B_1 - B_2$, and note that $M = -B$.
  Consequently, $M$ has finite first variation and hence $0$ quadratic variation.
  This implies $\E M(t)^2 = \E \qv{M}(t) = 0$ and hence $M = 0$ identically, which in turn implies $B = 0$, $B_1 = B_2$ and $M_1 = M_2$.
\end{proof}

Given an adapted process $\Delta$, interpret $X$ as the price of an asset, and $\Delta$ as our position on it.
(We could either be long, or short on the asset so $\Delta$ could be positive or negative.)

\begin{definition}
  We define the \remove{It\^o} integral of $\Delta$ with respect to $X$ by
  \begin{equation*}
    \int_0^T \Delta(t) \, dX(t)
      \defeq \int_0^T \Delta(t) b(t) \, dt 
	+ \int_0^T \Delta(t) \sigma(t) \, dW(t)\,.
  \end{equation*}
\end{definition}

As before, $\int_0^T \Delta \, dX$ represents the winnings or profit obtained using the trading strategy $\Delta$.

\begin{remark}
  Note that the first integral on the right $\int_0^T \Delta(t) b(t) \, dt$ is a regular Riemann integral, and the second one is an It\^o integral.
  Recall that It\^o integrals with respect to Brownian motion (i.e.\ integrals of the form $\int_0^t \Delta(s) \, dW(s)$ are martingales).
  Integrals with respect to a general process $X$ are only guaranteed to be martingales if $X$ itself is a martingale (i.e.\ $b = 0$), or if the integrand is $0$.
\end{remark}

\begin{remark}
  If we define $I_P$ by
  \begin{equation*}
    I_P(T)
      = \sum_{i=0}^{n-1} \Delta(t_i) (X(t_{i+1}) - X(t_i))
	+ \Delta(t_n) (X(T) - X(t_n))
    \quad\text{if } T \in [t_n, t_{n+1})\,,
  \end{equation*}
  then $I_P$ converges to the integral $\int_0^T \Delta(t) \, d X(t)$ defined above.
  This works in the same way as~Theorem~\ref{t:3ipconv}.
\end{remark}


Suppose now $f(t, x)$ is some function.
If $X$ is differentiable as a function of $t$ \emph{(which it most certainly is not)}, then the chain rule gives
\begin{align*}
  f(T, X(T)) - f(0, X(0))
    &= \int_0^T \partial_t \paren[\Big]{ f(t, X(t)) } \, dt
  \\
    &= \int_0^T \partial_t f( t, X(t)) \, dt
      + \int_0^T \partial_x f( t, X(t) ) \, \partial_t X(t) \, dt
  \\
    &= \int_0^T \partial_t f( t, X(t) ) \, dt
      + \int_0^T \partial_x f(t, X(t) ) \, dX(t)\,.
\end{align*}
It\^o process are \emph{almost never} differentiable as a function of time, and so the above has no chance of working.
It turns out, however, that for It\^o process you can make the above work by adding an \emph{It\^o correction} term.
This is the celebrated It\^o formula (more correctly the It\^o-Doeblin\footnote{%
  W. Doeblin was a French-German mathematician who was drafted for military service during the second world war.
  During the war he wrote down his mathematical work and sent it in a sealed envelope to the French Academy of Sciences, because he did not want it to ``fall into the wrong hands''.
  When he was about to be captured by the Germans he burnt his mathematical notes and killed himself.

  The sealed envelope was opened in 2000 which revealed that he had a treatment of stochastic Calculus that was essentially equivalent to It\^o's.
  In posthumous recognition, It\^o's formula is now referred to as the It\^o-Doeblin formula by many authors.
}
formula).

\begin{theorem}[It\^o formula, aka It\^o-Doeblin formula]
  If $f = f(t, x)$ is $C^{1,2}$ function\footnote{%
    Recall a function $f = f(t, x)$ is said to be $C^{1,2}$ if it is $C^1$ in $t$ (i.e.\ differentiable with respect to $t$ and $\partial_t f$ is continuous), and $C^2$ in $x$ (i.e.\ twice differentiable with respect to $x$ and $\partial_x f$, $\partial_x^2 f$ are both continuous).
  }
  then
  \begin{multline}\label{e:3ito}
    f(T, X(T)) - f(0, X(0))
      = \int_0^T \partial_t f( t, X(t) ) \, dt
	+ \int_0^T \partial_x f(t, X(t) ) \, dX(t)
    \\
	+ \frac{1}{2} \int_0^T \partial_x^2 f(t, X(t) \, d\qv{X}(t)\,.
  \end{multline}
\end{theorem}

\begin{remark}
  To clarify notation, $\partial_t f( t, X(t))$ means the following: differentiate~$f(t, x)$ with respect to $t$ (treating $x$ as a constant), and then substitute $x = X(t)$.
  Similarly $\partial_x f( t, X(t))$ means differentiate $f(t, x)$ with respect to $x$, and then substitute $x = X(t)$.
  Finally $\partial_x^2 f(t, X(t))$ means take the second derivative of the function $f(t, x)$ with respect to $x$, and the substitute $x = X(t)$.
\end{remark}
\begin{remark}
  In short hand differential form, this is written as
  \begin{multline}\label{e:3itoD}
    \tag{\ref{e:3ito}$'$}
    df(t, X(t)) = \partial_t f(t, X(t)) \, dt + \partial_x f(t, X(t)) \, dX(t)
    \\
      + \frac{1}{2} \partial_x^2 f(t, X(t)) \, d\qv{X}(t)\,.
  \end{multline}
  The term $\frac{1}{2} \partial_x^2 f \, d\qv{X}(t)$ is an ``extra'' term, and is often referred to as the \emph{It\^o correction term}.
  The It\^o formula is simply a version of the \emph{chain rule} for stochastic processes.
  %For brevity many write
  %\begin{equation*}
  %  df(t, X(t)) = \partial_t f \, dt + \partial_x f \, dX(t) + \frac{1}{2} \partial_x^2 f \, d\qv{X}(t)\,.
  %\end{equation*}
\end{remark}

\begin{remark}
  Substituting what we know about $X$ from~\eqref{e:3X} and~\eqref{e:3qvX} we see that~\eqref{e:3ito} becomes
  \begin{multline*}
    f(T, X(T)) - f(0, X(0))
      = \int_0^T \paren[\big]{ \partial_t f( t, X(t) )
	+ \partial_x f(t, X(t) ) b(t)} \, dt
    \\
      \mathbin+ \int_0^T \partial_x f(t, X(t)) \sigma(t) \, dW(t)
	+ \frac{1}{2} \int_0^T \partial_x^2 f(t, X(t)) \, \sigma(t)^2 \, dt\,.
  \end{multline*}
  The second integral on the right is an It\^o integral (and hence a martingale).
  The other integrals are regular Riemann integrals which yield processes of finite variation.
\end{remark}

While a complete rigorous proof of the It\^o formula is technical, and beyond the scope of this course, we provide a quick heuristic argument that illustrates the main idea clearly.

\begin{proof}[Intuition behind the It\^o formula]
  Suppose that the function $f$ is only a function of $x$ and doesn't depend on $t$, and $X$ is a standard Brownian motion (i.e.\ $b = 0$ and $\sigma = 1$).
  In this case proving It\^o's formula reduces to proving
  \begin{equation*}
    f(W(T)) - f(W(0)) = \int_0^T f'(W(t)) \, dW(t) + \frac{1}{2} \int_0^T f''(W(t)) \, dt \,.
  \end{equation*}

  Let $P = \set{0 = t_0 < t_1 < \cdots < t_n = T}$ be a partition of $[0, T]$.
  Taylor expanding $f$ to second order gives
  \begin{multline}\label{e:3itoProof1}
    f(W(T)) - f(W(0))
      = \sum_{i=0}^{n-1} f(W(t_{i+1})) - f(W(t_i))
    \\
      = \sum_{i=0}^{n-1} f'(W(t_i)) (W(t_{i+1}) - W(t_i))
	+ \frac{1}{2} \sum_{i=0}^{n-1} f''(W(t_i)) (W( t_{i+1}) - W(t_i))^2
    \\
	+ \frac{1}{2} \sum_{i=0}^{n-1} o\paren[\big]{ (W( t_{i+1}) - W(t_i))^2 } \,,
  \end{multline}
  where the last sum on the right is the remainder from the Taylor expansion.

  Note the first sum on the right of~\eqref{e:3itoProof1} converges to the It\^o integral
  \begin{equation*}
    \int_0^T f'(W(t)) \, dW(t)\,.
  \end{equation*}
  For the second sum on the right of~\eqref{e:3itoProof1}, note
  \begin{multline*}
    f''(W(t_i)) (W( t_{i+1}) - W(t_i))^2
      = f''(W(t_i)) (t_{i+1} - t_i)
    \\
	+ f''(W(t_i))\brak[\big]{(W( t_{i+1}) - W(t_i))^2 - (t_{i+1} - t_i)}
  \end{multline*}
  After summing over $i$, first term on the right converges to the Riemann integral $\int_0^T f''(W(t)) \, dt$.
  The second term on the right is similar to what we had when computing the quadratic variation of $W$.
  The variance of $(W( t_{i+1}) - W(t_i))^2 - (t_{i+1} - t_i)$ is of order $(t_{i+1} - t_i)^2$.
  Thus we expect that the second term above, when summed over $i$, converges to $0$.

  Finally each summand in the remainder term (the last term on the right of~\eqref{e:3itoProof1}) is smaller than $(W(t_{i+1}) - W(t_i))^2$.
  (If, for instance, $f$ is three times continuously differentiable in $x$, then each summand in the remainder term is of order $(W(t_{i+1}) - W(t_i))^3$.)
  Consequently, when summed over~$i$ this should converge to $0$ 
\end{proof}

\section{A few examples using It\^o's formula}

Technically, as soon as you know It\^o's formula you can ``jump right in'' and derive the Black-Scholes equation.
However, because of the importance of It\^o's formula, we work out a few simpler examples first.

\begin{example}
  Compute the quadratic variation of $W(t)^2$.
\end{example}
\begin{sol}
  Let $f(t, x) = x^2$.
  Then, by It\^o's formula,
  \begin{align*}
    d \paren[\big]{ W(t)^2 }
      &= d f(t, W(t))
    \\
      &= \partial_t f( t, W(t)) \, dt + \partial_x f (t, W(t)) \, dW(t)
	+ \frac{1}{2} \partial_x^2 f(t, W(t)) \, dt
    \\
      &= 2 W(t) \, dW(t) + dt\,.
  \end{align*}
  Or, in integral form,
  \begin{equation*}
    W(T)^2 - W(0)^2
      = W(T)^2
      = 2 \int_0^T W(t) \, dW(t) + T\,.
  \end{equation*}
  Now the second term on the right has finite first variation, and won't affect our computations for quadratic variation.
  The first term is an martingale whose quadratic variation is $\int_0^T W(t)^2 \, dt$, and so
  \begin{equation*}
    \qv{W^2}(T) = 4 \int_0^T W(t)^2 \, dt \,.
    \qedhere
  \end{equation*}
\end{sol}
\begin{remark}
  Note the above also tells you
  \begin{equation*}
    2 \int_0^T W(t) \, dW(t) = W(T)^2 - T \,.
  \end{equation*}
\end{remark}

\begin{example}
  Let $M(t) = W(t)$ and $N(t) = W(t)^2 - t$.
  We know $M$ and $N$ are martingales.
  Is $M N$ a martingale?
\end{example}
\begin{sol}
  Note $M(t) N(t) = W(t)^3 - t W(t)$.
  By It\^o's formula,
  \begin{equation*}
    d(MN) = - W(t) \, dt + (3 W(t)^2 - t) \, dW(t) + 3 W(t) \, dt \,.
  \end{equation*}
  Or in integral form
  \begin{equation*}
    M(t) N(t) = \int_0^t 2 W(s) \, ds + \int_0^t (3 W(s)^2 - s) \, dW(s)\,.
  \end{equation*}
  Now the second integral on the right is a martingale, but the first integral most certainly is not.
  So $MN$ can not be a martingale.
\end{sol}
\begin{remark}
  Note, above we changed the integration variable from $t$ to $s$.
  This is perfectly legal -- the variable with which you integrate with respect to is a dummy variable (just line regular Riemann integrals) and you can replace it what your favourite (unused!) symbol.
\end{remark}
\begin{remark}
  It's worth pointing out that the It\^o integral $\int_0^t \Delta(s) \, dW(s)$ is always a martingale (under the finiteness condition~\eqref{e:3Dsq}).
  However, the Riemann integral $\int_0^t b(s) \, ds$ is only a martingale if $b = 0$ identically.
\end{remark}

\begin{proposition}
  If $f = f(t, x)$ is $C^{1,2}_b$ then the process
  \begin{equation*}
    M(t) \defeq f(t, W(t))
      - \int_0^t \paren[\Big]{
	  \partial_t f(s, W(s)) + \frac{1}{2} \partial_x^2 f(s, W(s))
	} \, ds
  \end{equation*}
  is a martingale.
\end{proposition}
\begin{remark}
  We'd seen this earlier, and the proof involved computing the conditional expectations directly and checking an algebraic identity involving the density of the normal distribution.
  With It\^o's formula, the proof is ``immediate''.
\end{remark}
\begin{proof}
  By It\^o's formula (in integral form)
  \begin{align*}
    \MoveEqLeft
    f(t, W(t)) - f(0, W(0))
    \\
      &= \int_0^t \partial_t f(s, W(s)) \, ds
	+ \int_0^t \partial_x f( s, W(s)) \, dW(s)
	+ \frac{1}{2} \int_0^t \partial_x^2 f(s, W(s)) \, ds
    \\
      &= \int_0^t \paren[\Big]{
	      \partial_t f(s, W(s))
	      + \frac{1}{2} \partial_x^2 f(s, W(s))
	    } \, ds
	  + \int_0^t \partial_x f( s, W(s)) \, dW(s)\,.
  \end{align*}
  Substituting this we see
  \begin{equation*}
    M(t) = f(0, W(0))
	  + \int_0^t \partial_x f( s, W(s)) \, dW(s)\,,
  \end{equation*}
  which is a martingale.
\end{proof}
\begin{remark}
  Note we said $f \in C^{1,2}_b$ to ``cover our bases''.
  Recall for It\^o integrals to be martingales, we need the finiteness condition~\eqref{e:3Dsq} to hold.
  This will certainly be the case if $\partial_x f$ is bounded, which is why we made this assumption.
  The result above is of course true under much more general assumptions.
\end{remark}

\begin{example}
  Let $X(t) = t \sin( W(t) )$. Is $X^2 - \qv{X}$ a martingale?
\end{example}
\begin{sol}
  Let $f(t, x) = t \sin(x)$.
  Observe $X(t) = f(t, W(t))$, $\partial_t f = \sin x$, $\partial_x f = t \cos x$, and $\partial_x^2 f = - t \sin x$.
  Thus by It\^o's formula,
  \begin{multline*}
    dX(t)
      = \partial_t f( t, W(t)) \, dt + \partial_x f (t, W(t)) \, dW(t)
	+ \frac{1}{2} \partial_x^2 f(t, W(t)) \, d\qv{W}(t)
    \\
      = \sin(W(t)) \, dt + t \cos( W(t)) \, dW(t) - \frac{1}{2} t \sin(W(t)) \, dt\,,
  \end{multline*}
  and so
  \begin{equation*}
    d\qv{X}(t) = t^2 \cos^2(W(t)) \, dt\,.
  \end{equation*}
  Now let $g(x) = x^2$ and apply It\^o's formula to compute $d g(X(t))$.
  This gives
  \begin{equation*}
    d X(t)^2
      = 2 X(t) \, dX(t) + d\qv{X}(t)
  \end{equation*}
  and so
  \begin{multline*}
    d( X(t)^2 - \qv{X} )
      = 2 X(t) \, dX(t)
    \\
      = 2t \sin(t) \paren[\big]{ \sin(W(t)) - \frac{t \sin(W(t)) }{2} } \, dt
	+ 2t \sin(t) \paren[\big]{ t \cos(W(t)) } \, dW(t)\,.
  \end{multline*}
  Since the $dt$ term above isn't $0$, $X(t)^2 - \qv{X}$ can not be a martingale.
\end{sol}

Recall we said earlier (Theorem~\ref{t:3qv1}) that for any martingale $M$, $M^2 - \qv{M}$ is a martingale.
In the above example $X$ is not a martingale, and so there is no contradiction when we show that $X^2 - \qv{X}$ is not a martingale.
If $M$ is a martingale, It\^o's formula can be used to ``prove''%
\footnote{%
  We used the fact that $M^2 - \qv{M}$ is a martingale crucially in the construction of It\^o integrals, and hence in proving It\^o's formula.
  Thus proving $M^2 - \qv{M}$ is a martingale using the It\^o's formula is circular and not a valid proof.
  It is however instructive, and helps with building intuition, which is why it is presented here.%
}
that $M^2 - \qv{M}$ is a martingale.

\begin{proposition}
  Let $M(t)= \int_0^t \sigma(s) \, dW(s)$.
  Then $M^2 - \qv{M}$ is a martingale.
\end{proposition}
\begin{proof}
  Let $N(t) = M(t)^2 - \qv{M}(t)$.
  Observe that by It\^o's formula,
  \begin{equation*}
    d( M(t)^2 ) = 2 M(t) \, dM(t) + d\qv{M}(t)\,.
  \end{equation*}
  Hence
  \begin{equation*}
    d N = 2 M(t) \, dM(t) + d\qv{M}(t) - d\qv{M}(t)
      = 2M(t) \sigma(t) \, dW(t)\,.
  \end{equation*}
  Since there is no ``$dt$'' term and It\^o integrals are martingales, $N$ is a martingale.
\end{proof}

\section{Review Problems}

\begin{problem}
  If $0 \leq r < s < t$, compute $\E \paren[\big]{ W(r) W(s) W(t)}$.
\end{problem}

\begin{problem}
  Define the processes $X, Y, Z$ by
  \begin{equation*}
    X(t) = \int_0^{W(t)} e^{-s^2} \, ds\,,
    \quad
    Y(t) = \exp\paren[\Big]{ \int_0^t W(s) \, ds }\,,
    \quad
    Z(t) = \frac{t}{X(t)}\,.
  \end{equation*}
  Decompose each of these processes as the sum of a martingale and a process of finite first variation.
  What is the quadratic variation of each of these processes?
\end{problem}

\begin{problem}
  Define the processes $X, Y$ by
  \begin{equation*}
    X(t) \defeq \int_0^t W(s) \, ds\,, \quad
    Y(t) \defeq \int_0^t W(s) \, dW(s)\,.
  \end{equation*}
  Given $0 \leq s < t$, compute the conditional expectations
  $\E( X(t) \given \mathcal F_s )$, 
  and
  $\E( Y(t) \given \mathcal F_s )$.
\end{problem}

\begin{problem}
  Let $\displaystyle M(t) = \int_0^t W(s) \, dW(s)$.
  Find a function $f$ such that
  \begin{equation*}
    E(t) \defeq \exp\paren[\Big]{ M(t) - \int_0^t f(s, W(s)) \, ds }
  \end{equation*}
  is a martingale.
\end{problem}
\begin{problem}
  Suppose $\sigma = \sigma(t)$ is a deterministic (i.e.\ non-random) process, and $X$ is the It\^o process defined by
  \begin{equation*}
    X(t) = \int_0^t \sigma(u) \, dW(u)\,.
  \end{equation*}
  \vspace{-.7\baselineskip}
  \begin{parts}
    \item
      Given $\lambda, s, t \in \R$ with $0 \leq s < t$ compute $\E \paren{ e^{\lambda (X(t) - X(s))} \given \mathcal F_s}$.

    \item
      If $r \leq s$ compute $\E \exp\paren{\lambda X(r) + \mu(X(t) - X(s))}$.

    \item
      What is the joint distribution of $(X(r), X(t) - X(s))$?

    \item
      \emph{(L\'evy's criterion)}
      If $\sigma(u) = \pm 1$ for all $u$, then show that $X$ is a standard Brownian motion.
  \end{parts}
\end{problem}
\begin{problem}
  Define the process $X, Y$ by
  \begin{equation*}
    X = \int_0^t s \, dW(s)\,,
    \quad
    Y = \int_0^t W(s) \, ds\,.
  \end{equation*}
  Find a formula for $\E X(t)^n$ and $\E Y(t)^n$ for any $n \in \N$.
\end{problem}
\begin{problem}
  Let $\displaystyle M(t) = \int_0^t W(s) \, dW(s)$.
  For $s < t$, is $M(t) - M(s)$ independent of $\mathcal F_s$?
  Justify.
\end{problem}

\begin{problem}
  Determine whether the following identities are true or false, and justify your answer.
  \begin{parts}
    \item
      $\displaystyle e^{2t} \sin(2 W(t)) = 2 \int_0^t e^{2s} \cos(2 W(s)) \, dW(s)$.

    \item
      $\displaystyle \abs{W(t)} = \int_0^t \sign(W(s)) \, dW(s)$.
      (Recall $\sign(x) = 1$ if $x > 0$, $\sign(x) = -1$ if $x < 0$ and $\sign(x) = 0$ if $x = 0$.)
  \end{parts}
\end{problem}
\section{The Black Scholes Merton equation (unfinished).}

The price of an asset with a constant rate of return~$\alpha$ is given by
\begin{equation*}
  dS(t) = \alpha S(t) \, dt\,.
\end{equation*}
To account for noisy fluctuations we model stock prices by adding the term $\sigma S(t) \, dW(t)$ to the above:
\begin{equation}\label{e:3gbm}
  dS(t) = \alpha S(t) \, dt + \sigma dW(t)\,.
\end{equation}
The parameter $\alpha$ is called the \emph{mean return rate} or the \emph{percentage drift}, and the parameter $\sigma$ is called the \emph{volatility} or the \emph{percentage volatility}.

\begin{definition}
  A stochastic process $S$ satisfying~\eqref{e:3gbm} above is called a \emph{geometric Brownian motion}.
\end{definition}

The reason $S$ is called a geometric Brownian motion is as follows.
Set $Y = \ln S$ and observe
\begin{equation*}
  dY(t) = \frac{1}{S(t)} \, dS(t) - \frac{1}{2 S(t)^2} \, d\qv{S}(t)
    = \paren[\Big]{\alpha - \frac{\sigma^2}{2}} \, dt + \sigma \, dW(t)\,.
\end{equation*}
If $\alpha = \sigma^2/2$ then $Y = \ln S$ is simply a Brownian motion.

We remark, however, that our application of It\^o's formula above is not completely justified.
Indeed, the function $f(x) = \ln x$ is \emph{not} differentiable at $x = 0$, and It\^o's formula requires $f$ to be at least $C^2$.
The reason the application of It\^o's formula here is valid is because the process $S$ \emph{never} hits the point $x = 0$, and at all other points the function $f$ is infinitely differentiable.

The above also gives us an explicit formula for $S$.
Indeed,
\begin{equation*}
  \ln\paren[\Big]{ \frac{S(t)}{S(0)} }
    = \paren[\Big]{\alpha - \frac{\sigma^2}{2}} t + \sigma W(t)\,,
\end{equation*}
and so
\begin{equation}\label{e:3gbmFor}
  S(t) = S(0) \exp\paren[\Big]{
    \paren[\Big]{\alpha - \frac{\sigma^2}{2}} t + \sigma W(t)
  }\,.
\end{equation}

Now consider a European call option with strike price $K$ and maturity time $T$.
This is a security that allows you the option (not obligation) to buy~$S$ at price $K$ and time $T$.
Clearly the price of this option at time $T$ is $(S(T) - K)^+$.
Our aim is to compute the \emph{arbitrage free}%
\footnote{
  In an arbitrage free market, we say $p$ is the arbitrage free price of a non traded security if given the opportunity to trade the security at price $p$, the market is still arbitrage free.
  (Recall a financial market is said to be \emph{arbitrage free} if there doesn't exist a self-financing portfolio $X$ with $X(0) = 0$ such that at some $t > 0$ we have $X(t) \geq 0$ and $\P(X(t) > 0) > 0$.)%
}
price of such an option at time $t < T$.

Black and Scholes realised that the price of this option at time $t$ should only depend on the asset price $S(t)$, and the current time $t$ (or more precisely, the time to maturity $T - t$), and of course the model parameters $\alpha, \sigma$.
In particular, the option price does not depend on the price history of $S$.

\begin{theorem}\label{t:3bsm}
  Suppose we have an arbitrage free financial market consisting of a money market account with constant return rate $r$, and a risky asset whose price is given by $S$.
  Consider a European call option with strike price $K$ and maturity $T$.
  \begin{enumerate}
    \item\label{p:3bsm1}
      If $c = c(x, t)$ is a function such that at any time $t \leq T$, the arbitrage free price of this option is $c(t, S(t))$, then
      \begin{alignat}{2}
	\span\label{e:3BS}
	  \partial_t c + r x \partial_x c + \frac{\sigma^2 x^2}{2} \partial_x^2 c - r c
	  = 0
	  &\qquad& x > 0,\ t < T\,,
	\\
	\span\label{e:3cBC}
	  c(t, 0) = 0
	  && t \leq T\,,
	\\
	\span\label{e:3TC}
	  c(T, x) = (x - K)^+
	  && x \geq 0\,.
      \end{alignat}

    \item\label{p:3bsm2}
      Conversely, if $c$ satisfies~\eqref{e:3BS}--\eqref{e:3TC} then $c(t, S(t))$ is the arbitrage free price of this option at any time $t \leq T$.
  \end{enumerate}
\end{theorem}
\begin{remark}
  Since $\alpha, \sigma$ and $T$ are fixed, we suppress the explicit dependence of $c$ on these quantities.
\end{remark}
\begin{remark}
  The above result assumes the following:
  \begin{enumerate}
    \item
      The market is \emph{frictionless} (i.e.\ there are no transaction costs).
    \item
      The asset is liquid and fractional quantities of it can be traded.
    \item
      The borrowing and lending rate are both $r$.
  \end{enumerate}
\end{remark}
\begin{remark}
  Even though the asset price $S(t)$ is random, the function $c$ is a deterministic (non-random) function.
  The option price, however, is $c(t, S(t))$, which is certainly random.
\end{remark}
\begin{remark}
  Equation~\eqref{e:3BS}--\eqref{e:3TC} are the \emph{Black Scholes Merton PDE}.
  This is a \emph{partial differential equation}, which is a differential equation involving derivatives with respect to more than one variable.
  Equation~\eqref{e:3BS} governs the evolution of $c$ for $x \in (0, \infty)$ and $t < T$.
  Equation~\eqref{e:3TC} specifies the terminal condition at $t = T$, and equation~\eqref{e:3cBC} specifies a boundary condition at $x = 0$.

  To be completely correct, one also needs to add a boundary condition as $x \to \infty$ to the system~\eqref{e:3BS}--\eqref{e:3TC}.
  When $x$ is very large, the call option is deep in the money, and will very likely end in the money.
  In this case the price fluctuations of $S$ are small in comparison, price of the option should roughly be the discounted cash value of $(x - K)^+$.
  Thus a boundary condition at $x = \infty$ can be obtained by supplementing~\eqref{e:3cBC} with
  \begin{gather}\label{e:3cBCp}
    \tag{\ref{e:3cBC}$'$}
    \lim_{x \to \infty} \paren[\big]{
      c(t, x) - (x - K e^{-r(T-t)})
    } = 0\,.
  \end{gather}
\end{remark}
\begin{remark}
  The system~\eqref{e:3BS}--\eqref{e:3TC} can be solved explicitly using standard calculus by substituting~$y = \ln x$ and converting it into the \emph{heat equation}, for which the solution is explicitly known.
  This gives the Black-Scholes-Merton formula
  \begin{equation}\label{e:3c}
    c(t, x) = x N( d_+(T - t, x) ) - Ke^{-r (T-t)} N( d_-(T-t, x) )
  \end{equation}
  where
  \begin{equation}\label{e:3dpm}
    d_\pm( \tau, x ) \defeq
      \frac{1}{\sigma \sqrt{\tau}}\paren[\Big]{
	\ln\paren[\Big]{\frac{x}{K}}
	  + \paren[\Big]{ r \pm \frac{\sigma^2}{2} } \tau
      }\,,
  \end{equation}
  and
  \begin{equation}\label{e:3N}
    N(x) \defeq \frac{1}{\sqrt{2\pi}} \int_0^x e^{-y^2 / 2} \, dy\,,
  \end{equation}
  is the CDF of a standard normal variable.

  Even if you're unfamiliar with the techniques involved in arriving at the solution above, you can certainly check that the function $c$ given by~\eqref{e:3c}--\eqref{e:3dpm} above satisfies~\eqref{e:3BS}--\eqref{e:3TC}.
  Indeed, this is a direct calculation that only involves patience and a careful application of the chain rule.
  We will, however, derive~\eqref{e:3c}--\eqref{e:3dpm} later using risk neutral measures.
\end{remark}



We will prove Theorem~\ref{t:3bsm} by using a \emph{replicating portfolio}.
This is a portfolio (consisting of cash and the risky asset) that has exactly the same cash flow at maturity as the European call option that needs to be priced.
%We compute this price by creating a replicating portfolio with the same cash flow.
%That is, if $X(t)$ is the value of the replicating portfolio at time $t$, we will arrange 
Specifically, let $X(t)$ be the value of the replicating portfolio and $\Delta(t)$ be the number of shares of the asset held.
The remaining $X(t) - S(t) \Delta(t)$ will be invested in the money market account with return rate $r$.
(It is possible that $\Delta(t) S(t) > X(t)$, in which means we borrow money from the money market account to invest in stock.)
For a replicating portfolio, the trading strategy $\Delta$ should be chosen in a manner that ensures that we have the same cash flow as the European call option.
That is, we must have $X(T) = (S(T) - K)^+ = c(T, S(T))$.
Now the arbitrage free price is precisely the value of this portfolio.

\begin{remark}
  Through the course of the proof we will see that given the function $c$, the number of shares of $S$ the replicating portfolio should hold is given by the \emph{delta hedging rule}
  \begin{equation}\label{e:3DH}
    \Delta(t) = \partial_x c(t, S(t)) \,.
  \end{equation}
\end{remark}
\begin{remark}
  Note that there is no $\alpha$ dependence in the system~\eqref{e:3BS}--\eqref{e:3TC}, and consequently the formula~\eqref{e:3c} does not depend on~$\alpha$.
  At first sight, this might appear surprising.
  (In fact, Black and Scholes had a hard time getting the original paper published because the community couldn't believe that the final formula is independent of~$\alpha$.)
  The fact that~\eqref{e:3c} is independent of $\alpha$ can be heuristically explained by the fact that the replicating portfolio also holds the same asset: thus a high mean return rate will help both an investor holding a call option and an investor holding the replicating portfolio.
  (Of course this isn't the entire story, as one has to actually write down the dependence and check that an investor holding the call option benefits exactly as much as an investor holding the replicating portfolio.
  This is done below.)
\end{remark}
\begin{proof}[Proof of Theorem~\ref{t:3bsm} part~\ref{p:3bsm1}]
  If $c(t, S(t))$ is the arbitrage free price, then, by definition 
  \begin{equation}\label{e:3ceqx}
    c(t, S(t)) = X(t)\,,
  \end{equation}
  where $X(t)$ is the value of a replicating portfolio.
  %that holds~$\Delta(t)$ shares of the asset and the remaining $X(t) - \Delta(t) S(t)$ in the money market account.
  Since our portfolio holds $\Delta(t)$ shares of $S$ and $X(t) - \Delta(t) S(t)$ in a money market account, the evolution of the value of this portfolio is given by
  \begin{align*}
    dX(t) &= \Delta(t) \, dS(t) + r \paren[\big]{X(t) - \Delta(t) S(t)} \, dt
    \\
      &= \paren[\big]{ r X(t) + (\alpha - r) \Delta(t) S(t) } \, dt
	+ \sigma \Delta(t) S(t) \, dW(t)\,.
  \end{align*}
  Also, by It\^o's formula we compute
  \begin{align*}
    d c(t, S(t))
      &= \partial_t c(t, S(t)) \, dt + \partial_x c(t, S(t)) \, dS(t) + \frac{1}{2} \partial_x^2 c(t, S(t)) \, d\qv{S}(t)\,,
    \\
      &= \paren[\Big]{\partial_t c + \alpha S \partial_x c + \frac{1}{2} \sigma^2 S^2 \partial_x^2 c} \, dt
	+ \partial_x c\, \sigma S \, dW(t)
  \end{align*}
  where we suppressed the $(t, S(t))$ argument in the last line above for convenience.

  Equating $dc(t, S(t)) = dX(t)$ gives
  \begin{multline*}
    \paren[\big]{ r X(t) + (\alpha - r) \Delta(t) S(t) } \, dt
      + \sigma \Delta(t) S(t) \, dW(t)
    \\
      = \paren[\Big]{\partial_t c + \alpha S \partial_x c + \frac{1}{2} \sigma^2 S^2 \partial_x^2 c} \, dt
	+ \partial_x c\, \sigma S \, dW(t)\,.
  \end{multline*}
  Using uniqueness of the semi-martingale decomposition (Proposition~\ref{p:3smuniq}) we can equate the $dW$ and the $dt$ terms respectively.
  Equating the $dW$ terms gives the delta hedging rule~\eqref{e:3DH}.
  Writing $S(t) = x$ for convenience, equating the $dt$ terms and using~\eqref{e:3ceqx} gives~\eqref{e:3BS}.
  Since the payout of the option is $(S(T) - K)^+$ at maturity, equation~\eqref{e:3TC} is clearly satisfied.

  Finally if $S(t_0) = 0$ at one particular time, then we must have $S(t) = 0$ at all times, otherwise we would have an arbitrage opportunity.
  (This can be checked directly from the formula~\eqref{e:3gbmFor} of course.)
  Consequently the fair price of the option when $S = 0$ is $0$, giving the \emph{boundary condition}~\eqref{e:3cBC}.
  Hence~\eqref{e:3BS}--\eqref{e:3TC} are all satisfied, finishing the proof.
\end{proof}
\begin{proof}[Proof of Theorem~\ref{t:3bsm} part~\ref{p:3bsm2}]
  For the converse, we suppose $c$ satisfies the system~\eqref{e:3BS}--\eqref{e:3TC}.
  Choose $\Delta(t)$ by the delta hedging rule~\eqref{e:3DH}, and let $X$ be a portfolio with initial value $X(0) = c(0, S(0))$ that holds $\Delta(t)$ shares of the asset at time $t$ and the remaining $X(t) - \Delta(t) S(t)$ in cash.
  We claim that $X$ is a replicating portfolio (i.e.\ $X(T) = (S(T) - K)^+$ almost surely) and $X(t) = c(t, S(t))$ for all $t \leq T$.
  Once this is established $c(t, S(t))$ is the arbitrage free price as desired.

  To show $X$ is a replicating portfolio, first claim that $X(t) = c(t, S(t))$ for all $t < T$.
  To see this, let $Y(t) = e^{-r t} X(t)$ be the discounted value of $X$.
  (That is, $Y(t)$ is the value of $X(t)$ converted to cash at time $t = 0$.)
  By It\^o's formula, we compute
  \begin{align*}
    dY(t) &= -r Y(t) \, dt + e^{-rt} dX(t)
    \\
      &= e^{-rt} (\alpha - r) \Delta(t) S(t) \, dt
	  + e^{-rt} \sigma \Delta(t) S(t) \, dW(t)\,.
  \end{align*}

  Similarly, using It\^o's formula, we compute
  \begin{equation*}
    d \paren[\big]{ e^{-rt} c(t, S(t)) }
      = e^{-rt} \paren[\Big]{-r c + \partial_t c + \alpha S \partial_x c + \frac{1}{2} \sigma^2 S^2 \partial_x^2 c} \, dt
	+ \partial_x c\, \sigma S \, dW(t)\,.
  \end{equation*}
  Using~\eqref{e:3BS} this gives
  \begin{equation*}
    d \paren[\big]{ e^{-rt} c(t, S(t)) }
      = e^{-rt} (\alpha - r) S \partial_x c \, dt
	+ \partial_x c\, \sigma S \, dW(t)
      = dY(t)\,,
  \end{equation*}
  since $\Delta(t) = \partial_x c(t, S(t))$ by choice.
  This forces
  \begin{multline*}
    e^{-rt} X(t)
      = X(0) + \int_0^t dY(s)
      = X(0) + \int_0^t d\paren[\big]{ e^{-rs} c(s, S(s)) }
    \\
      = X(0) + e^{-rt}c(t, S(t)) - c(0, S(0))
      = e^{-rt}c(t, S(t))\,,
  \end{multline*}
  since we chose $X(0) = c(0, S(0))$.
  This forces $X(t) = c(t, S(t))$ for all $t < T$, and by continuity also for $t = T$.
  Since $c(T, S(T)) = (S(T) - K)^+$ we have $X(T) = (S(T) - K)^+$ showing $X$ is a replicating portfolio, concluding the proof.
\end{proof}
\begin{remark}
  In order for the application of It\^o's formula to be valid above, we need $c \in C^{1,2}$.
  This is certainly false at time $T$, since $c(T, x) = (x - K)^+$ which is not even differentiable, let alone twice continuously differentiable.
  However, if $c$ satisfies the system~\eqref{e:3BS}--\eqref{e:3TC}, then it turns out that for every $t < T$ the function $c$ will be infinitely differentiable with respect to $x$.
  This is why our proof first shows that $c(t, S(t)) = X(t)$ for $t < T$ and not directly that $c(t, S(t)) = X(t)$ for all $t \leq T$.
\end{remark}
\begin{remark}[Put Call Parity]
  The same argument can be used to compute the arbitrage free price of European put options (i.e.\ the option to sell at the strike price, instead of buying).
  However, once the price of the price of a call option is computed, the \emph{put call parity} can be used to compute the price of a put.

  Explicitly let $p = p(t, x)$ be a function such that at any time $t \leq T$, $p(t, S(t))$ is the arbitrage free price of a European put option with strike price $K$.
  Consider a portfolio $X$ that is long a call and short a put (i.e.\ buy one call, and sell one put).
  The value of this portfolio at time $t < T$ is
  \begin{equation*}
    X(t) = c(t, S(t)) - p(t, S(t))\,
  \end{equation*}
  and at maturity we have%
  \footnote{
    A \emph{forward contract} requires the holder to buy the asset at price $K$ at maturity.
    The value of this contract at maturity is exactly $S(T) - K$, and so a portfolio that is long a put and short a call has exactly the same cash flow as a forward contract.%
  }
  \begin{equation*}
    X(T) = (S(T) - K)^+ - (S(T) - K)^- = S(T) - K\,.
  \end{equation*}
  This payoff can be replicated using a portfolio that holds one share of the asset and borrows $K e^{-rT}$ in cash (with return rate $r$) at time $0$.
  Thus, in an arbitrage free market, we should have
  \begin{equation*}
    c(t, S(t)) - p(t, S(t)) = X(t) = S(t) - K e^{-r(T-t)}\,.
  \end{equation*}
  Writing $x$ for $S(t)$ this gives the \emph{put call parity} relation
  \begin{equation*}
    c(t, x) - p(t, x) = x - K e^{-r(T-t)}\,.
  \end{equation*}
  Using this the price of a put can be computed from the price of a call.
\end{remark}

We now turn to understanding properties of $c$.
%We will prove this by using the formula~\eqref{e:3c} and differentiating.
The partial derivatives of $c$ with respect to $t$ and $x$ measure the sensitivity of the option price to changes in the time to maturity and spot price of the asset respectively.
These are called ``the Greeks'':
\begin{asparaenum}
  \item
    The \emph{delta} is defined to be $\partial_x c$, and is given by
    \begin{equation*}
      \partial_x c = N(d_+) + x N'(d_+) d_+' - Ke^{r \tau} N'(d_-)d_-'\,.
    \end{equation*}
    where $\tau = T - t$ is the time to maturity.
    Recall~$d_\pm = d_\pm( \tau, x )$, and we suppressed the $(\tau, x)$ argument above for notational convenience.
    Using the formulae~\eqref{e:3c}--\eqref{e:3N} one can verify 
    \begin{equation*}
      d_+' = d_-' = \frac{1}{x \sigma \sqrt{\tau}}
      \qquad\text{and}\qquad
      x N'(d_+)  = Ke^{r \tau} N'(d_-)\,,
    \end{equation*}
    and hence the delta is given by
    \begin{equation*}
      \partial_x c = N(d_+)\,.
    \end{equation*}
    Recall that the \emph{delta hedging rule} (equation~\eqref{e:3DH}) explicitly tells you that the replicating portfolio should hold precisely $\partial_x c(t, S(t))$ shares of the risky asset and the remainder in cash.

  \item
    The \emph{gamma} is defined to be $\partial_x^2 c$, and is given by
    \begin{equation*}
      \partial_x^2 c
	= N'(d_+) d_+' 
	= \frac{1}{x \sigma \sqrt{2 \pi \tau}}
	  \exp\paren[\Big]{ \frac{-d_+^2}{2} } \,.
    \end{equation*}

  \item
    Finally the \emph{theta} is defined to be $\partial_t c$, and simplifies to
    \begin{equation*}
      \partial_t c = -r K e^{-r \tau} N(d_-)
	- \frac{\sigma x}{2 \sqrt{\tau}} N'(d_+)
    \end{equation*}
\end{asparaenum}

\begin{proposition}
  The function $c(t, x)$ is convex and increasing as a function of $x$, and is \emph{decreasing} as a function of $t$.
\end{proposition}
\begin{proof}
  This follows immediately from the fact that $\partial_x c > 0$, $\partial_x^2 c > 0$ and $\partial_t c < 0$.
\end{proof}

\begin{remark}[Hedging a short call]
  Suppose you sell a call option valued at $c(t, x)$, and want to create a replicating portfolio.
  The delta hedging rule calls for $x \partial_x c(t, x)$ of the portfolio to be invested in the asset, and the rest in the money market account.
  Consequently the value of your money market account is
  \begin{equation*}
    c(t, x) - x \partial_x c
      = x N(d_+) - K e^{-r \tau} N(d_-) - x N(d_+)
      = - K e^{-r \tau} N(d_-) < 0\,.
  \end{equation*}
  Thus to properly hedge a short call you will have to borrow from the money market account and invest it in the asset.
  As $t \to T$ you will end up selling the asset if $x < K$, and buying it if $x < K$, so that at maturity you will hold the asset if $x > K$ and not hold it if $x < K$.
  To hedge a long call you do the opposite.
\end{remark}
\begin{remark}[Delta neutral and long gamma]
  TODO
\end{remark}
\end{document}
