%&subfile
\begin{document}
\ifstandalonechapter\setcounter{chapter}{2}\fi
\chapter{Stochastic Integration (unfinished)}

\section{Motivation}
Suppose $\Delta(t)$ is your position at time $t$ on a security whose price is $S(t)$.
If you only trade this security at times $0 = t_0 < t_1 < t_2 < \cdots < t_n = T$, then the value of your winnings up to time $T$ is exactly
\begin{equation*}
  X(t_n) = \sum_{i=0}^{n-1} \Delta(t_i) ( S(t_{i+1}) - S(t_i) )
\end{equation*}
If you are trading this continuously in time, you'd expect that a ``simple'' limiting procedure should show that your winnings are given by the \emph{Riemann-Stieltjes} integral:
\begin{equation*}%\label{e:3stLim}
  X(T) = 
    \lim_{\norm{P} \to 0}
      \sum_{i=0}^{n-1} \Delta(t_i) \paren{ S( t_{i+1} ) - S( t_i ) }
    = \int_0^T \Delta(t) \, dS(t) \,.
\end{equation*}
Here $P = \set{0 = t_0 < \cdots < t_n = T}$ is a partition of $[0, T]$, and $\norm{P} = \max\set{ t_{i+1} - t_i }$.

%The reason we have $X(\xi_i)$ above, and not $X(t_i)$ is because the standard definition of the \emph{Riemann-Stieltjes} allows you to choose \emph{arbitrary} times in the partition sub-intervals $[t_i, t_{i+1}]$.
%In our context, this doesn't sit right: If you trade at time $t_i$, you should do so using only information available to you up to time $t_i$ (i.e.\ be $\mathcal F_{t_i}$-measurable), and your winnings are $X(t_i) (S(t_{i+1}) - S(t_i) )$ and not $X(\xi_i) (S(t_{i+1}) - S(t_i) )$.
%Indeed, it turns out that if $S$ is a continuous martingale, then the limit of the form~\eqref{e:3stLim} \emph{will not} exist in general.

This has been well studied by mathematicians, and it is well known that for the above limiting procedure to ``work directly'', you need $S$ to have finite \emph{first variation}.
Recall, the \emph{first variation} of a function is defined to be
\begin{equation*}
  V_{[0,T]}(S) \defeq \lim_{\norm{P} \to 0} \sum_{i=0}^{n-1} \abs{S(t_{i+1}) - S(t_i)}\,.
\end{equation*}
It turns out that almost \emph{any} continuous martingale~$S$ \emph{will not} have finite first variation.
Thus to define integrals with respect to martingales, one has to do something `clever'.
It turns out that if $X$ is adapted and $S$ is an martingale, then the above limiting procedure works, and this was carried out by It\^o (and independently by Doeblin).

\section{The First Variation of Brownian motion}

We begin by showing that the first variation of Brownian motion is infinite.
\begin{proposition}
  If $W$ is a standard Brownian motion, and $T > 0$ then
  \begin{equation*}
    \lim_{n \to \infty} \E \sum_{k=0}^{n-1}
      \abs[\Big]{
	W\paren[\Big]{\frac{k+1}{n}}
	- W\paren[\Big]{\frac{k}{n}}
      } = \infty\,.
  \end{equation*}
\end{proposition}
\begin{remark}
  In fact
  \begin{equation*}
    \lim_{n \to \infty} \sum_{k=0}^{n-1}
      \abs[\Big]{
	W\paren[\Big]{\frac{k+1}{n}}
	- W\paren[\Big]{\frac{k}{n}}
      } = \infty
      \quad\text{almost surely,}
  \end{equation*}
  but this won't be necessary for our purposes.
\end{remark}
\begin{proof}
  Since $W( (k+1)/n ) - W(k/n) \sim N(0, 1/n)$ we know
  \begin{equation*}
    \E \abs[\Big]{
	W\paren[\Big]{\frac{k+1}{n}}
	- W\paren[\Big]{\frac{k}{n}}
      }
      = \int_\R \abs{x} \, G\paren[\Big]{ \frac{1}{n}, x } \, dx
      = \frac{C}{\sqrt{n}}\,,
  \end{equation*}
  where
  \begin{equation*}
     C = \int_\R \abs{y} e^{-y^2/2} \, \frac{dy}{\sqrt{2\pi}}
      = \E \abs{N(0, 1)} \,.
  \end{equation*}
  Consequently
  \begin{equation*}
    \sum_{k=0}^{n-1}
      \E
      \abs[\Big]{
	W\paren[\Big]{\frac{k+1}{n}}
	- W\paren[\Big]{\frac{k}{n}}
      }
      = \frac{C n}{\sqrt{n}}
      \xrightarrow{n \to \infty} \infty\,.
      \qedhere
  \end{equation*}
\end{proof}

\section{Quadratic Variation}

It turns out that the \emph{second} variation of any \emph{square integrable} martingale is almost surely finite, and this is the key step in constructing the It\^o integral.
\begin{definition}
  Let $M$ be any process.
  We define the \emph{quadratic variation} of $M$, denoted by $\qv{M}$ by
  \begin{equation*}
    \qv{M}(T) = \lim_{\norm{P} \to 0} (M(t_{i+1}) - M(t_i))^2\,,
  \end{equation*}
  where $P = \set{0 = t_1 < t_1 \cdots < t_n = T}$ is a partition of $[0, T]$.
\end{definition}

\begin{proposition}
  If $W$ is a standard Brownian motion, then $\qv{W}(T) = T$ almost surely.
\end{proposition}
\begin{proof}
  For simplicity, let's assume $t_i = Ti/n$.
  Note
  \begin{equation*}
    \sum_{i=0}^{n-1}
      \paren[\Big]{
	W\paren[\Big]{\frac{(i+1)T}{n}}
	- W\paren[\Big]{\frac{iT}{n}}
      }^2 - T
      = \sum_{i=0}^{n-1}
	  \xi_i\,,
  \end{equation*}
  where
  \begin{equation*}
    \xi_i \defeq
	    \paren[\Big]{
	      W\paren[\Big]{\frac{(i+1)T}{n}}
	      - W\paren[\Big]{\frac{iT}{n}}
	    }^2 - \frac{T}{n}\,.
  \end{equation*}
  Note that $\xi_i$'s are i.i.d.\ with distribution $N(0, T/n)^2 - T/n$, and hence
  \begin{equation*}
    \E \xi_i = 0
    \qquad\text{and}\qquad
    \var \xi_i = \frac{T^2 (\E N(0,1)^4 - 1)}{n^2}.
  \end{equation*}
  Consequently
  \begin{equation*}
    \var\paren[\Big]{
      \sum_{i=0}^{n-1} \xi_i
    } = \frac{T^2 (\E N(0,1)^4 - 1)}{n} \xrightarrow{n \to \infty} 0\,,
  \end{equation*}
  which shows
  \begin{equation*}
    \sum_{i=0}^{n-1}
      \paren[\Big]{
	W\paren[\Big]{\frac{(i+1)T}{n}}
	- W\paren[\Big]{\frac{iT}{n}}
      }^2 - T
      = \sum_{i=0}^{n-1}
	  \xi_i
      \xrightarrow{n \to \infty} 0 \,.
      \qedhere
  \end{equation*}
\end{proof}

\begin{corollary}
  The process $M(t) \defeq W(t)^2 - \qv{W}(t)$ is a martingale.
\end{corollary}
\begin{proof}
  We see
  \begin{multline*}
    \E( W(t)^2 - t \given \mathcal F_s )
      = \E ( (W(t) - W(s))^2  + 2 W(s) (W(t) - W(s)) + W(s)^2 \given \mathcal F_s ) - t
      \\
      = W(s)^2 - s\,
  \end{multline*}
  and hence $\E (M(t) \given \mathcal F_s ) = M(s)$.
\end{proof}

The above wasn't a co-incidence.
This property in fact characterizes the quadratic variation.
\begin{theorem}\label{t:qv1}
  Let $M$ be a continuous martingale with respect to a filtration~$\set{\mathcal F_t}$.
  Then $\E M(t)^2 < \infty$ if and only if $\E \qv{M}(t) < \infty$.
  In this case the process $M(t)^2 - \qv{M}(t)$ is also a martingale with respect to the same filtration, and hence $\E M(t)^2 - \E M(0)^2 = \E \qv{M}(t)$.
\end{theorem}

The above is in fact a characterization of the quadratic variation of martingales.

\begin{theorem}\label{t:qv2}
  If $A(t)$ is any continuous, increasing, adapted process such that $M(t)^2 - A(t)$ is a martingale, then $A = \qv{M}$.
\end{theorem}

The proof of these theorems are a bit technical and go beyond the scope of these notes.
The results themselves, however, are extremely important and will be used subsequently.

\begin{remark}
  The intuition to keep in mind about the first variation and the quadratic variation is the following.
  Divide the interval $[0, T]$ into $T/\delta t$ intervals of size $\delta t$.
  If $X$ has finite first variation, then on each subinterval $(k \delta t, (k+1) \delta t)$ the increment of $X$ should be of order $\delta t$.
  Thus adding $T / \delta t$ terms of order $\delta t$ will yield something finite.

  On the other hand if $X$ has finite quadratic variation, on each subinterval $(k \delta t, (k+1) \delta t)$ the increment of $X$ should be of order $\sqrt{\delta t}$, so that adding $T / \delta t$ terms of the \emph{square} of the increment yields something finite.

  Doing a quick check for Brownian motion, we see
  \begin{equation*}
    \E \abs{W(t + \delta t) - W(t)} = \sqrt{\delta t} \E \abs{N(0, 1)}
    \quad\text{and}\quad
    \E \abs{W(t + \delta t) - W(t)}^2 = \delta t\,,
  \end{equation*}
  in line with our intuition above.
\end{remark}

\begin{remark}
  If a process has finite first variation, its quadratic variation will necessarily be $0$.
  On the other hand, if a continuous process has finite (and non-zero) quadratic variation, its first variation will necessary be infinite.
\end{remark}


\section{Construction of the It\^o integral}

Let $W$ be a standard Brownian motion, $\set{\mathcal F_t}$ be the Brownian filtration and $\Delta$ be an adapted process.
We think of $\Delta(t)$ to represent our position at time $t$ on an asset whose spot price is $W(t)$.

\begin{lemma}\label{l:3dito}
Let $P = \set{0 = t_0 < t_1 < t_2 < \cdots}$ be an increasing sequence of times, and assume $\Delta$ is constant on $[t_i, t_{i+1})$ (i.e.\ the asset is only traded at times $t_0$, \dots, $t_n$).
Let $I_P(T)$, defined by
\begin{equation*}
  I_P(T)
    = \sum_{i=0}^{n-1} \Delta(t_i) (W(t_{i+1}) - W(t_i))
      + \Delta(t_n) (W(T) - W(t_n))
  \quad\text{if } T \in [t_n, t_{n+1})\,.
\end{equation*}
be your cumulative winnings up to time $T$.
Then,
\begin{equation}\label{e:3Eip2}
  \E I_P(T)^2
    = \E \brak[\Big]{ \sum_{i=0}^{n} \Delta(t_i)^2 (t_{i+1} - t_i)
      + \Delta(t_n)^2 (T - t_n) }
  \quad\text{if } T \in [t_n, t_{n+1})\,.
\end{equation}
Moreover, $I_P$ is a martingale and
\begin{equation}\label{e:3dqv}
  \qv{I_P}(T) = \sum_{i=0}^{n} \Delta(t_i)^2 (t_{i+1} - t_i)
      + \Delta(t_n)^2 (T - t_n)
  \quad\text{if } T \in [t_n, t_{n+1})\,.
\end{equation}
\end{lemma}

This lemma, as we will shortly see, is the key to the construction of stochastic integrals (called It\^o integrals).
\begin{proof}%[Proof of Lemma~\ref{l:3dito}]
  We first prove~\eqref{e:3Eip2} with $T = t_n$ for simplicity.
  Note
  \begin{multline}\label{e:3sum1}
    \E I_P(t_n)^2 
      = \sum_{i=0}^{n-1}
	  \E \Delta(t_i)^2 (W(t_{i+1}) - W(t_i))^2
    \\
	+ 2 \sum_{j=0}^{n-1}
	  \sum_{i=0}^{i-1}
	  \E \Delta(t_i) \Delta(t_j)
	    (W(t_{i+1}) - W(t_i))
	    (W(t_{j+1}) - W(t_j))
  \end{multline}
  By the tower property
  \begin{multline*}
    \E \Delta(t_i)^2 (W(t_{i+1}) - W(t_i))^2
      = \E \E \paren[\big]{ \Delta(t_i)^2 (W(t_{i+1}) - W(t_i))^2 \given \mathcal F_{t_i} }
    \\
      = \E \Delta(t_i)^2 \E \paren[\big]{ (W(t_{i+1}) - W(t_i))^2 \given \mathcal F_{t_i} }
      = \E \Delta(t_i)^2 (t_{i+1} - t_i)\,.
  \end{multline*}
  Similarly we compute
  \begin{align*}
    \MoveEqLeft
    \E \Delta(t_i) \Delta(t_j)
      (W(t_{i+1}) - W(t_i))
      (W(t_{j+1}) - W(t_j))
    \\
    &= \E \E \paren[\big]{ \Delta(t_i) \Delta(t_j)
      (W(t_{i+1}) - W(t_i))
      (W(t_{j+1}) - W(t_j)) \given \mathcal F_{t_j} }
    \\
    &= \E \Delta(t_i) \Delta(t_j)
	  (W(t_{i+1}) - W(t_i))
	  \E \paren[\big]{ (W(t_{j+1}) - W(t_j)) \given \mathcal F_{t_j} }
    = 0\,.
  \end{align*}
  Substituting these in~\eqref{e:3sum1} immediately yields~\eqref{e:3Eip2} for $t_n = T$.

  The proof of~\eqref{e:3dqv} is similar in spirit, but has a few more details to check.
  Let $A(t)$ be the right hand side of~\eqref{e:3dqv}.
  This is clearly a continuous, increasing, adapted process.
  Now the main idea is to use the tower property to check that the process $M(t)^2 - A(t)$ is a martingale, and then use Theorem~\ref{t:qv2}.
\end{proof}

Note that as $\norm{P} \to 0$, the right hand side of~\eqref{e:3dqv} converges to the standard Riemann integral $\int_0^T \Delta(t)^2 \, dt$.
It\^o realised he could use this to prove that $I_P$ itself converges, and the limit is now called the It\^o integral.

\begin{theorem}\label{t:3ipconv}
  If $\int_0^T \Delta(t)^2 \, dt < \infty$ almost surely, then as $\norm{P} \to 0$, the processes $I_P$ converge to a \emph{continuous process} $I$ denoted by
  \begin{equation*}
    I(T)
      \defeq \lim_{\norm{P} \to 0} I_P(T)\,
      \defeq \int_0^T \Delta(t) \, dW(t)\,.
  \end{equation*}
  This is known as the \emph{It\^o integral of $\Delta$ with respect to $W$}.
  If further
  \begin{equation}\label{e:3Dsq}
    \E \int_0^T \Delta(t)^2 \, dt < \infty\,,
  \end{equation}
  then the process $I(T)$ is a \emph{martingale} and the quadratic variation $\qv{I}$ satisfies
  \begin{equation*}
    \qv{I}(T) = \int_0^T \Delta(t)^2 \, dt
    \quad\text{almost surely.}
  \end{equation*}
\end{theorem}

\begin{remark}
  For the above to work, it is \emph{crucial} that $\Delta$ is adapted, and is sampled at the left endpoint of the time intervals.
  That is, the terms in the sum are $\Delta(t_i) (W(t_{i+1}) - W(t_i))$, and not $\Delta(t_{i+1}) (W(t_{i+1}) - W(t_i))$ or $\frac{1}{2} (\Delta(t_i) + \Delta(t_{i+1})) (W(t_{i+1}) - W(t_i))$, or something else.

  Usually if the process is not adapted, there is no meaningful way to make sense of the limit.
  If you sample at different points, it still works out (usually) but what you get is \emph{different} from the It\^o integral (one example is the Stratonovich integral). 
\end{remark}

\begin{corollary}[It\^o Isometry]
  If~\eqref{e:3Dsq} holds then
  \begin{equation*}
    \E \paren[\Big]{ \int_0^T \Delta(t) \, dW(t) }^2
      = \E \int_0^T \Delta(t)^2 \, dt\,.
  \end{equation*}
\end{corollary}

\begin{proposition}[Linearity]
  If $\Delta_1$ and $\Delta_2$ are two adapted processes, and $\alpha \in \R$, then
  \begin{equation*}
    \int_0^T (\Delta_1(t) + \alpha \Delta_2(t)) \, dW(t) 
      \leq \int_0^T \Delta_1(t) \, dW(t) + \alpha \int_0^T \Delta_2(t) \, dW(t)\,.
  \end{equation*}
\end{proposition}
\begin{remark}
  Positivity, however, is not preserved by It\^o integrals.
  Namely if $\Delta_1 \leq \Delta_2$, there is no reason to expect $\int_0^T \Delta_1(t) \, dW(t) \leq \int_0^T \Delta_2(t) \, dW(t)$.
  Indeed choosing $\Delta_1 = 0$ and $\Delta_2 = 1$ we see that we can not possibly have $0 = \int_0^T \Delta_1(t) \, dW(t)$ to be almost surely smaller than $W(T) = \int_0^T \Delta_2(t) \, dW(t)$.
\end{remark}


Recall, our starting point in these notes was modelling stock prices as \emph{geometric Brownian motions}, given by the equation
\begin{equation*}
  dS(t) = \alpha S(t) \, dt + \sigma S(t) \, dW(t)\,.
\end{equation*}
After constructing It\^o integrals, we are now in a position to describe what this means.
The above is simply shorthand for saying $S$ is a process that satisfies
\begin{equation*}
  S(T) - S(0) = \int_0^T \alpha S(t) \, dt + \int_0^T \sigma S(t) \, dW(t)\,.
\end{equation*}
The first integral on the right is a standard Riemann integral.
The second integral, representing the noisy fluctuations, is the It\^o integral we just constructed.

Note that the above is a little more complicated than the It\^o integrals we will study first, since the process $S$ (that we're trying to define) also appears as an integrand on the right hand side.
In general, such equations are called \emph{Stochastic differential equations}, and are extremely useful in many contexts.

\section{The It\^o formula (unfinished)}

Using the abstract ``limit'' definition of the It\^o integral, it is hard to compute examples.
For instance, what is
\begin{equation*}
  \int_0^T W(s) \, dW(s) \,?
\end{equation*}
This, as we will shortly, can be computed easily using the It\^o formula (also called the It\^o-Doeblin formula).

Suppose $b$ and $\sigma$ are adapted processes.
(In particular, they could but need not, be random).
Consider a process $X$ defined by
\begin{equation}\label{e:3X}
  X(T) = X(0) + \int_0^T b(t) \, dt + \int_0^T \sigma(t) \, dW(t)\,.
\end{equation}
Note the first integral $\int_0^T b(t) \, dt$ is a regular Riemann integral that can be done directly.
The second integral the It\^o integral we constructed in the previous section.

\begin{definition}
  The process $X$ is called an It\^o process if $X(0)$ is deterministic (not random) and for all $T \geq 0$,
  \begin{equation*}
    \E \int_0^T \sigma(t)^2 \, dt < \infty
    \qquad\text{and}\qquad
    \int_0^T b(t) \, dt < \infty\,.
  \end{equation*}
\end{definition}

\begin{remark}
  The shorthand notation for~\eqref{e:3X} is to write
  \begin{equation*}
    \tag{\ref{e:3X}$'$}
    dX(t) = b(t) \, dt + \sigma(t) \, dW(t)\,.
  \end{equation*}
\end{remark}

\begin{proposition}
  The quadratic variation of $X$ is
  \begin{equation}\label{e:3qvX}
    \qv{X}(T) = \int_0^T \sigma(t)^2 \, dt
    \quad\text{almost surely.}
  \end{equation}
\end{proposition}
\begin{proof}
  Define $B$ and $M$ by
  \begin{equation*}
    B(T) = \int_0^T b(t) \, dt
    \qquad\text{and}\qquad
    M(T) = \int_0^T \sigma(t) \, dW(t)\,
  \end{equation*}
  and observe
  \begin{multline}\label{e:3incX}
    (X(s + \delta t) - X(s))^2
      = (B(s+\delta t) - B(s))^2 
	+ (M( s + \delta t) - M(s))^2
	\\
	+ 2 \paren{ B(s + \delta t) - B(s) }
	    \paren{ M(s + \delta t) - M(s) }\,.
  \end{multline}

  The key point is that the increment in $B$ is of order $\delta t$, and the increments in $M$ are of order $\sqrt{\delta t}$.
  Indeed, note
  \begin{equation*}
    \abs{B(s + \delta t) - B(s)} = \abs[\Big]{ \int_s^{s + \delta t} b(t) \, dt}
      \leq (\delta t) \max \abs{b}\,,
  \end{equation*}
  showing that increments of $B$ are of order $\delta t$ as desired.
  We already know $M$ has finite quadratic variation, so already expect the increments of $M$ to be of order $\delta t$.

  Now to compute the quadratic variation of $X$, let $n = \floor{T / \delta t}$, $t_i = i \delta t$ and observe
  \begin{multline*}
    \sum_{i=0}^{n-1} \paren{ X(t_{i+1}) - X( t_i ) }^2
      = \sum_{i=0}^{n-1} \paren{ M(t_{i+1}) - M( t_i ) }^2
    \\
    \mathbin+ \sum_{i=0}^{n-1} \paren{ B(t_{i+1}) - B( t_i ) }^2
	+ 2\sum_{i=0}^{n-1}
	    \paren{ B(t_{i+1}) - B( t_i ) }
	    \paren{ M(t_{i+1}) - M( t_i ) }\,.
  \end{multline*}
  The first sum on the right converges (as $\delta t \to 0$) to $\qv{M}(T)$, which we know is exactly $\int_0^T \sigma(t)^2 \, dt$.
  Each term in the second sum is of order $(\delta t)^2$.
  Since there are $T / \delta t$ terms, the second sum converges to $0$.
  Similarly, each term in the third sum is of order $(\delta t)^{3/2}$ and so converges to $0$ as $\delta t \to 0$.
\end{proof}

\begin{remark}
  It's common to decompose $X = B + M$ where $M$ is a martingale and $B$ has finite first variation.
  Processes that can be decomposed in this form are called \emph{semi-martingales}, and the decomposition is unique.
  The process $M$ is called the martingale part of $X$, and $B$ is called the \emph{bounded variation} part of $X$.
\end{remark}

Given an adapted process $\Delta$, interpret $X$ as the price of an asset, and $\Delta$ as our position on it.
(We could either be long, or short on the asset so $\Delta$ could be positive or negative.)

\begin{definition}
  We define the It\^o integral of $\Delta$ with respect to $X$ by
  \begin{equation*}
    \int_0^T \Delta(t) \, dX(t)
      \defeq \int_0^T \Delta(t) b(t) \, dt 
	+ \int_0^T \Delta(t) \sigma(t) \, dW(t)\,.
  \end{equation*}
\end{definition}

As before, $\int_0^T \Delta \, dX$ represents the winnings or profit obtained using the trading strategy $\Delta$.

\begin{remark}
  Note that the first integral on the right $\int_0^T \Delta(t) b(t) \, dt$ is a regular Riemann integral, and the second one is an It\^o integral.
\end{remark}

\begin{remark}
  If we define $I_P$ by
  \begin{equation*}
    I_P(T)
      = \sum_{i=0}^{n-1} \Delta(t_i) (X(t_{i+1}) - X(t_i))
	+ \Delta(t_n) (X(T) - X(t_n))
    \quad\text{if } T \in [t_n, t_{n+1})\,,
  \end{equation*}
  then $I_P$ converges to the integral $\int_0^T \Delta(t) X(t) \, dt$ defined above.
  This works in the same way as~Theorem~\ref{t:3ipconv}.
\end{remark}


Suppose now $f(t, x)$ is some function.
If $X$ is differentiable as a function of $t$ \emph{(which it most certainly is not)}, then the chain rule gives
\begin{align*}
  f(T, X(T)) - f(0, X(0))
    &= \int_0^T \partial_t \paren[\Big]{ f(t, X(t)) } \, dt
  \\
    &= \int_0^T \partial_t f( t, X(t)) \, dt
      + \int_0^T \partial_x f( t, X(t) ) \, \partial_t X(t) \, dt
  \\
    &= \int_0^T \partial_t f( t, X(t) ) \, dt
      + \int_0^T \partial_x f(t, X(t) ) \, dX(t)\,.
\end{align*}
It\^o process are \emph{almost never} differentiable as a function of time, and so the above has no chance of working.
It turns out, however, that for It\^o process you can make the above work by adding an \emph{It\^o correction} term.
This is the celebrated It\^o formula (more correctly the It\^o-Doeblin\footnote{%
  W. Doeblin was a French-German mathematician who was drafted for military service during the second world war.
  During the war he wrote down his mathematical work and sent it in a sealed envelope to the French Academy of Sciences, because he did not want it to ``fall into the wrong hands''.
  When he was about to be captured by the Germans he burnt his mathematical notes and killed himself.

  The sealed envelope was opened in 2000 which revealed that he had a treatment of stochastic Calculus that was essentially equivalent to It\^o's.
  In posthumous recognition, It\^o's formula is now referred to as the It\^o-Doeblin formula by many authors.
}
formula).

\begin{theorem}[It\^o formula]
  Suppose $f = f(t, x)$ is a function that is continuously differentiable in $t$ and twice continuously differentiable in $x$, then
  \begin{multline}\label{e:3ito}
    f(T, X(T)) - f(0, X(0))
      = \int_0^T \partial_t f( t, X(t) ) \, dt
	+ \int_0^T \partial_x f(t, X(t) ) \, dX(t)
    \\
	+ \frac{1}{2} \int_0^T \partial_x^2 f(t, X(t) \, d\qv{X}(t)\,.
  \end{multline}
\end{theorem}

\begin{remark}
  In short hand differential form, this is written as
  \begin{equation}\label{e:3itoD}
    \tag{\ref{e:3ito}$'$}
    df(t, X(t)) = \partial_t f(t, X(t)) \, dt + \partial_x f(t, X(t)) \, dX(t)
      + \frac{1}{2} \partial_x^2 f(t, X(t)) \, d\qv{X}(t)\,.
  \end{equation}
  The term $\frac{1}{2} \partial_x^2 f \, d\qv{X}(t)$ is an ``extra'' term, and is often referred to as the \emph{It\^o correction term}.
  The It\^o formula is simply a version of the \emph{chain rule} for stochastic processes.
  %For brevity many write
  %\begin{equation*}
  %  df(t, X(t)) = \partial_t f \, dt + \partial_x f \, dX(t) + \frac{1}{2} \partial_x^2 f \, d\qv{X}(t)\,.
  %\end{equation*}
\end{remark}

\begin{remark}
  Substituting what we know about $X$ from~\eqref{e:3X} and~\eqref{e:3qvX} we see that~\eqref{e:3ito} becomes
  \begin{multline*}
    f(T, X(T)) - f(0, X(0))
      = \int_0^T \paren[\big]{ \partial_t f( t, X(t) )
	+ \partial_x f(t, X(t) ) b(t)} \, dt
    \\
      \mathbin+ \int_0^T \partial_x f(t, X(t)) \sigma(t) \, dW(t)
	+ \frac{1}{2} \int_0^T \partial_x^2 f(t, X(t) \, \sigma(t)^2 \, dt\,.
  \end{multline*}
  The second integral on the right is an It\^o integral (and hence a martingale).
  The other integrals are regular Riemann integrals which yield processes of finite variation.
\end{remark}

While a complete rigorous proof of the It\^o formula is technical, and beyond the scope of this course, we provide a quick heuristic argument here.

\begin{proof}[Intuition behind the It\^o formula]
\end{proof}

\begin{todo}
\begin{enumerate}
  \item Motivate the proof of It\^o's formula.
  \item Compute a few integrals
\end{enumerate}
\end{todo}
\end{document}
